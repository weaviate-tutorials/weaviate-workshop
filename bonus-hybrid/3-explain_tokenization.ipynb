{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Property, DataType, Tokenization, Configure\n",
    "from weaviate.classes.query import Filter\n",
    "\n",
    "client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_word Tokenization.WORD\n",
      "text_lowercase Tokenization.LOWERCASE\n",
      "text_whitespace Tokenization.WHITESPACE\n",
      "text_field Tokenization.FIELD\n"
     ]
    }
   ],
   "source": [
    "tkn_options = [\n",
    "    Tokenization.WORD,\n",
    "    Tokenization.LOWERCASE,\n",
    "    Tokenization.WHITESPACE,\n",
    "    Tokenization.FIELD,\n",
    "]\n",
    "\n",
    "properties = [\n",
    "    Property(\n",
    "        name=f\"text_word\",\n",
    "        data_type=DataType.TEXT,\n",
    "        tokenization=Tokenization.WORD\n",
    "    ),\n",
    "    Property(\n",
    "        name=f\"text_lowercase\",\n",
    "        data_type=DataType.TEXT,\n",
    "        tokenization=Tokenization.LOWERCASE\n",
    "    ),\n",
    "    Property(\n",
    "        name=f\"text_whitespace\",\n",
    "        data_type=DataType.TEXT,\n",
    "        tokenization=Tokenization.WHITESPACE\n",
    "    ),\n",
    "    Property(\n",
    "        name=f\"text_field\",\n",
    "        data_type=DataType.TEXT,\n",
    "        tokenization=Tokenization.FIELD\n",
    "    ),\n",
    "]\n",
    "\n",
    "for p in properties:\n",
    "    print(p.name, p.tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TokenExample collection\n"
     ]
    }
   ],
   "source": [
    "client.collections.delete(\"TokenExample\")\n",
    "\n",
    "collection = client.collections.create(\n",
    "    name=\"TokenExample\",\n",
    "    properties=properties,\n",
    "    vectorizer_config=Configure.Vectorizer.none()\n",
    ")\n",
    "\n",
    "print(\"Created TokenExample collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_word': 'Lois & Clark: The New Adventures of Superman', 'text_lowercase': 'Lois & Clark: The New Adventures of Superman', 'text_whitespace': 'Lois & Clark: The New Adventures of Superman', 'text_field': 'Lois & Clark: The New Adventures of Superman'}\n",
      "{'text_word': 'SW1A 1AA', 'text_lowercase': 'SW1A 1AA', 'text_whitespace': 'SW1A 1AA', 'text_field': 'SW1A 1AA'}\n",
      "{'text_word': '15-30', 'text_lowercase': '15-30', 'text_whitespace': '15-30', 'text_field': '15-30'}\n",
      "{'text_word': '30-15', 'text_lowercase': '30-15', 'text_whitespace': '30-15', 'text_field': '30-15'}\n",
      "{'text_word': 'Beyoncé - Single Ladies (Put a Ring on It)', 'text_lowercase': 'Beyoncé - Single Ladies (Put a Ring on It)', 'text_whitespace': 'Beyoncé - Single Ladies (Put a Ring on It)', 'text_field': 'Beyoncé - Single Ladies (Put a Ring on It)'}\n"
     ]
    }
   ],
   "source": [
    "property_names = [p.name for p in properties]\n",
    "\n",
    "for phrase in [\n",
    "    \"Lois & Clark: The New Adventures of Superman\",\n",
    "    \"SW1A 1AA\",\n",
    "    \"15-30\",\n",
    "    \"30-15\",\n",
    "    \"Beyoncé - Single Ladies (Put a Ring on It)\",\n",
    "]:\n",
    "    obj_properties = {name: phrase for name in property_names}\n",
    "    print(obj_properties)\n",
    "    collection.data.insert(\n",
    "        properties=obj_properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_test_query(query_term):\n",
    "    print(f\"\\nHits for: '{query_term}'\")\n",
    "\n",
    "    # run a query on each property\n",
    "    for name in property_names:\n",
    "        response = collection.query.fetch_objects(\n",
    "            filters=Filter.by_property(name).like(query_term),\n",
    "            limit=5\n",
    "        )\n",
    "\n",
    "        if len(response.objects) > 0:\n",
    "            for obj in response.objects:\n",
    "                print(f\"'{obj.properties[name]}' found in {name}\")\n",
    "        else:\n",
    "            print(f\"No matches for {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hits for: 'Superman'\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_word\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_lowercase\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_whitespace\n",
      "No matches for text_field\n"
     ]
    }
   ],
   "source": [
    "token_test_query(\"Superman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hits for: 'SUPERman'\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_word\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_lowercase\n",
      "No matches for text_whitespace\n",
      "No matches for text_field\n",
      "\n",
      "Hits for: 'Super man'\n",
      "No matches for text_word\n",
      "No matches for text_lowercase\n",
      "No matches for text_whitespace\n",
      "No matches for text_field\n"
     ]
    }
   ],
   "source": [
    "token_test_query(\"SUPERman\")\n",
    "token_test_query(\"Super man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hits for: 'Lois & Superman'\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_word\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_lowercase\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_whitespace\n",
      "No matches for text_field\n"
     ]
    }
   ],
   "source": [
    "token_test_query(\"Lois & Superman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hits for: 'Lois & Clark'\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_word\n",
      "No matches for text_lowercase\n",
      "No matches for text_whitespace\n",
      "No matches for text_field\n",
      "\n",
      "Hits for: 'Lois & Clark:'\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_word\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_lowercase\n",
      "'Lois & Clark: The New Adventures of Superman' found in text_whitespace\n",
      "No matches for text_field\n"
     ]
    }
   ],
   "source": [
    "token_test_query(\"Lois & Clark\")\n",
    "token_test_query(\"Lois & Clark:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hits for: 'SW1A 1AA'\n",
      "'SW1A 1AA' found in text_word\n",
      "'SW1A 1AA' found in text_lowercase\n",
      "'SW1A 1AA' found in text_whitespace\n",
      "'SW1A 1AA' found in text_field\n",
      "\n",
      "Hits for: '1AA'\n",
      "'SW1A 1AA' found in text_word\n",
      "'SW1A 1AA' found in text_lowercase\n",
      "'SW1A 1AA' found in text_whitespace\n",
      "No matches for text_field\n",
      "\n",
      "Hits for: '1AA SW1A'\n",
      "'SW1A 1AA' found in text_word\n",
      "'SW1A 1AA' found in text_lowercase\n",
      "'SW1A 1AA' found in text_whitespace\n",
      "No matches for text_field\n"
     ]
    }
   ],
   "source": [
    "token_test_query(\"SW1A 1AA\")\n",
    "token_test_query(\"1AA\")\n",
    "token_test_query(\"1AA SW1A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hits for: '15-30'\n",
      "'15-30' found in text_word\n",
      "'30-15' found in text_word\n",
      "'15-30' found in text_lowercase\n",
      "'15-30' found in text_whitespace\n",
      "'15-30' found in text_field\n",
      "\n",
      "Hits for: '30-15'\n",
      "'15-30' found in text_word\n",
      "'30-15' found in text_word\n",
      "'30-15' found in text_lowercase\n",
      "'30-15' found in text_whitespace\n",
      "'30-15' found in text_field\n"
     ]
    }
   ],
   "source": [
    "token_test_query(\"15-30\")\n",
    "\n",
    "token_test_query(\"30-15\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
