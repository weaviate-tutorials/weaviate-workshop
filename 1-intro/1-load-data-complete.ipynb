{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Weaviate Python Client v4\n",
    "> This notebook was created with Weaviate `1.25` and the Weaviate Client `4.6`\n",
    "\n",
    "Run the below command to install the latest version of the Weaviate Python Client v4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U weaviate-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Weaviate\n",
    "\n",
    "Weaviate offers 3 deployment options:\n",
    "* Embedded\n",
    "* Self-hosted - with Docker Compose\n",
    "* Cloud deployment - [Weaviate Cloud Service](https://console.weaviate.cloud/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate\n",
    "\n",
    "* If you are new to OpenAI, register at [https://platform.openai.com](https://platform.openai.com/) and head to [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys) to create your API key.\n",
    "* If you are new to Cohere, register at [https://cohere.com](https://https://cohere.com) and head to [https://dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys) to create your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, json\n",
    "\n",
    "# Connect with Weaviate Embedded\n",
    "# client = weaviate.connect_to_embedded(\n",
    "#     version=\"1.25.7\",\n",
    "# )\n",
    "\n",
    "# Connect to the local instance deployed with Docker Compose\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, os, json\n",
    "\n",
    "# Connect to a cloud instance of Weaviate (with WCS)\n",
    "# client = weaviate.connect_to_weaviate_cloud(\n",
    "#     cluster_url=os.getenv(\"WORKSHOP_DEMO_URL\"),\n",
    "#     auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WORKSHOP_DEMO_KEY_ADMIN\")),\n",
    "\n",
    "#     headers={        \n",
    "#         \"X-AWS-Access-Key\": os.getenv(\"AWS_ACCESS_KEY\"),\n",
    "#         \"X-AWS-Secret-Key\": os.getenv(\"AWS_SECRET_KEY\"),\n",
    "#     }\n",
    "# )\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a collection\n",
    "[Weaviate Docs - collection creation and configuration](https://weaviate.io/developers/weaviate/configuration/schema-configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure\n",
    "\n",
    "if client.collections.exists(\"Jeopardy\"):\n",
    "    client.collections.delete(\"Jeopardy\")\n",
    "\n",
    "# Create a collection here - with Ollama vectorizer\n",
    "client.collections.create(\n",
    "    name=\"Jeopardy\",\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_ollama(\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "        model=\"snowflake-arctic-embed\",  # The model to use, e.g. \"nomic-embed-text\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure\n",
    "\n",
    "# if client.collections.exists(\"Jeopardy\"):\n",
    "#     client.collections.delete(\"Jeopardy\")\n",
    "\n",
    "# Create a collection here\n",
    "# client.collections.create(\n",
    "#     name=\"Jeopardy\",\n",
    "\n",
    "    # Option 1 - Use Titan Embed model \n",
    "    # vectorizer_config=Configure.Vectorizer.text2vec_aws(\n",
    "    #     model=\"amazon.titan-embed-text-v1\",\n",
    "    #     # region=\"eu-central-1\",\n",
    "    #     region=\"us-east-1\",\n",
    "    # ),\n",
    "\n",
    "    # # Option 2 - Use Cohere embedding model through AWS Bedrock\n",
    "    # vectorizer_config=Configure.Vectorizer.text2vec_aws(\n",
    "    #     model=\"cohere.embed-english-v3\",\n",
    "    #     region=\"us-east-1\"\n",
    "    # ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_10 = json.load(open(\"./jeopardy_tiny.json\"))\n",
    "\n",
    "print(json.dumps(data_10, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Many\n",
    "[Weaviate Docs - insert many](https://weaviate.io/developers/weaviate/manage-data/import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data\n",
    "jeopardy = client.collections.get(\"Jeopardy\")\n",
    "jeopardy.data.insert_many(data_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data preview\n",
    "jeopardy = client.collections.get(\"Jeopardy\")\n",
    "response = jeopardy.query.fetch_objects(limit=4)\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.uuid, item.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data preview - with vectors\n",
    "jeopardy = client.collections.get(\"Jeopardy\")\n",
    "response = jeopardy.query.fetch_objects(\n",
    "    limit=4,\n",
    "    include_vector=True\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties)\n",
    "    print(item.vector, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super quick query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = jeopardy.query.near_text(\n",
    "    query=\"African animals\",\n",
    "    # query=\"weather\",\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a collection with a Generative module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new collection with 1k objects and vectorizer and generative model\n",
    "\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "# if client.collections.exists(\"Questions\"):\n",
    "#     client.collections.delete(\"Questions\")\n",
    "\n",
    "# Create a collection here - with Cohere as a vectorizer\n",
    "client.collections.create(\n",
    "    name=\"Questions\",\n",
    "\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_ollama(\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "        model=\"snowflake-arctic-embed\",  # The model to use, e.g. \"nomic-embed-text\"\n",
    "    ),\n",
    "\n",
    "    generative_config=Configure.Generative.ollama(\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "        model=\"llama3\"  # The model to use, e.g. \"phi3\", or \"mistral\", \"command-r-plus\", \"gemma\"\n",
    "    ),\n",
    "\n",
    "    properties=[  # Define properties (Optional)\n",
    "        Property(name=\"question\", data_type=DataType.TEXT),\n",
    "        Property(name=\"answer\", data_type=DataType.TEXT),\n",
    "        Property(name=\"category\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        Property(name=\"round\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        Property(name=\"points\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"airDate\", data_type=DataType.DATE),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with named vectors\n",
    "> Key benefit `source_properties` â€“ to tell Weaviate, which properties to vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Property, Configure, DataType\n",
    "\n",
    "# if client.collections.exists(\"Questions\"):\n",
    "#     client.collections.delete(\"Questions\")\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Questions\",\n",
    "\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_ollama(\n",
    "            name=\"title_vector\",\n",
    "            source_properties=[\"question\", \"answer\"],\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "            model=\"snowflake-arctic-embed\",  # The model to use, e.g. \"nomic-embed-text\"\n",
    "        )\n",
    "    ],\n",
    "\n",
    "    generative_config=Configure.Generative.ollama(\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "        model=\"llama3\"  # The model to use, e.g. \"phi3\", or \"mistral\", \"command-r-plus\", \"gemma\"\n",
    "    ),\n",
    "\n",
    "    properties=[  # Define properties (Optional)\n",
    "        Property(name=\"question\", data_type=DataType.TEXT),\n",
    "        Property(name=\"answer\", data_type=DataType.TEXT),\n",
    "        Property(name=\"category\", data_type=DataType.TEXT),\n",
    "        Property(name=\"round\", data_type=DataType.TEXT),\n",
    "        Property(name=\"points\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"airDate\", data_type=DataType.DATE),\n",
    "    ],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data - 1k objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_1k = json.load(open(\"./jeopardy_1k.json\"))\n",
    "\n",
    "print(json.dumps(data_1k, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data\n",
    "questions = client.collections.get(\"Questions\")\n",
    "\n",
    "with questions.batch.dynamic() as batch:\n",
    "    for item in data_1k:\n",
    "        batch.add_object(item)\n",
    "\n",
    "if(len(questions.batch.failed_objects)>0):\n",
    "    print(\"Import complete with errors\")\n",
    "    for err in questions.batch.failed_objects:\n",
    "        print(err)\n",
    "else:\n",
    "    print(\"Import complete with no errors\")\n",
    "\n",
    "# questions.data.insert_many(data_1k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
