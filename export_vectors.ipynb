{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Export Vectorized Data to Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WEAVIATE_KEY = os.getenv(\"WEAVIATE_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_URL = os.getenv(\"OPENAI_URL\")\n",
    "\n",
    "print(f\"Weaviate Key: {WEAVIATE_KEY}\")\n",
    "print(f\"OpenAI URL: {OPENAI_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Connect to Weaviate\n",
    "client = weaviate.connect_to_local(\n",
    "    host=\"localhost\",\n",
    "    port=8080,\n",
    "    grpc_port=50051,\n",
    "    auth_credentials=Auth.api_key(WEAVIATE_KEY),\n",
    "    headers={\n",
    "        \"X-OpenAI-Api-Key\": OPENAI_API_KEY,\n",
    "        \"X-OpenAI-BaseURL\": OPENAI_URL\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Connected: {client.is_ready()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Wikinew collection and check count\n",
    "wiki = client.collections.get(\"Wikinew\")\n",
    "total_count = len(wiki)\n",
    "print(f\"Found {total_count} items in Wikinew collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"wiki-data/weaviate/nomic-embed-text\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Exporting to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export remaining data starting from offset 50,000\n",
    "all_data = []\n",
    "\n",
    "print(\"Exporting remaining data using fetch_objects with offset...\")\n",
    "try:\n",
    "    # Start from where we left off (50,000) and get the rest\n",
    "    remaining_items = total_count - 50000\n",
    "    print(f\"Attempting to get {remaining_items} remaining items...\")\n",
    "    \n",
    "    batch_size = 100\n",
    "    start_offset = 50000\n",
    "    \n",
    "    for i in tqdm(range(0, remaining_items, batch_size), desc=\"Fetching remaining batches\"):\n",
    "        current_offset = start_offset + i\n",
    "        try:\n",
    "            response = wiki.query.fetch_objects(\n",
    "                limit=min(batch_size, remaining_items - i),\n",
    "                offset=current_offset,\n",
    "                include_vector=True\n",
    "            )\n",
    "\n",
    "            batch_count = 0\n",
    "            for obj in response.objects:\n",
    "                all_data.append({\n",
    "                    \"title\": obj.properties[\"title\"],\n",
    "                    \"text\": obj.properties[\"text\"],\n",
    "                    \"wiki_id\": obj.properties[\"wiki_id\"],\n",
    "                    \"url\": obj.properties[\"url\"],\n",
    "                    \"vector\": obj.vector[\"default\"]\n",
    "                })\n",
    "                batch_count += 1\n",
    "            \n",
    "            if batch_count == 0:\n",
    "                print(f\"No more items at offset {current_offset}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error at offset {current_offset}: {e}\")\n",
    "            print(f\"Collected {len(all_data)} items before hitting limit\")\n",
    "            break\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error during export: {e}\")\n",
    "    print(f\"Managed to collect {len(all_data)} items\")\n",
    "\n",
    "print(f\"Collected {len(all_data)} additional items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and save as parquet files\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Split into files of 25k each (like original)\n",
    "items_per_file = 25000\n",
    "total_files = (len(df) + items_per_file - 1) // items_per_file\n",
    "\n",
    "print(f\"Splitting into {total_files} files...\")\n",
    "for i in range(total_files):\n",
    "    start_idx = i * items_per_file\n",
    "    end_idx = min((i + 1) * items_per_file, len(df))\n",
    "\n",
    "    file_df = df.iloc[start_idx:end_idx]\n",
    "    filename = f\"{output_dir}/{i+1:04d}.parquet\"\n",
    "    file_df.to_parquet(filename, index=False)\n",
    "    print(f\"✓ Saved {filename} with {len(file_df)} items\")\n",
    "\n",
    "print(f\"✅ Export completed! {total_files} files saved to {output_dir}/\")\n",
    "print(f\"Total items exported: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the exported data\n",
    "test_file = f\"{output_dir}/0001.parquet\"\n",
    "test_df = pd.read_parquet(test_file)\n",
    "print(f\"Sample file shape: {test_df.shape}\")\n",
    "print(f\"Columns: {test_df.columns.tolist()}\")\n",
    "print(f\"Vector dimensions: {len(test_df['vector'].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "print(\"✓ Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
