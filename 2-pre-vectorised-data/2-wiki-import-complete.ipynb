{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data with Vectors\n",
    "\n",
    "## Get keys and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "weaviate_url = os.getenv(\"WEAVIATE_URL\")\n",
    "# openai_key = os.getenv(\"OPENAI_API_KEY\") # we don't need the OpenAI key for this\n",
    "openai_url = os.getenv(\"OPENAI_URL\")\n",
    "\n",
    "print(weaviate_url, openai_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    host=weaviate_url\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure\n",
    "\n",
    "def create_wiki_collection():\n",
    "    if client.collections.exists(\"Wiki\"):\n",
    "        client.collections.delete(\"Wiki\")\n",
    "\n",
    "    # Create a collection here - with OpenAI vectorizer and define source properties\n",
    "    client.collections.create(\n",
    "        name=\"Wiki\",\n",
    "\n",
    "        vectorizer_config=[\n",
    "            Configure.NamedVectors.text2vec_openai(\n",
    "                name=\"main_vector\",\n",
    "\n",
    "                model=\"text-embedding-3-small\",\n",
    "                base_url=openai_url,\n",
    "\n",
    "                # we don't need source_properties in this example - unless we expect to add data without providing vectors\n",
    "                # source_properties=['title', 'text']\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "create_wiki_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def prepare_dataset():\n",
    "    # return load_dataset('parquet', data_files={'train': ['../dataset/openai/*.parquet']}, split=\"train\")\n",
    "    return load_dataset(\"weaviate/wiki-sample\", \"openai-text-embedding-3-small\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Test\n",
    "<!-- The parquet files should be located in \"datasets/openai\". -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:01,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Unicode', 'text': \"The Unicode Standard includes more than just the base code. Alongside the character encodings, the Consortium's official publication includes a wide variety of details about the scripts and how to display them: normalization rules, decomposition, collation, rendering, and bidirectional text display order for multilingual texts, and so on.\", 'wiki_id': '20231101.simple_64846_4', 'url': 'https://simple.wikipedia.org/wiki/Unicode'}\n",
      "{'title': 'Book of Genesis', 'text': 'The people of the world attempted to build a high tower (Tower of Babel) to show the power of mankind and to reach God. God felt insulted and gave people different languages to prevent the tower from ever being finished.', 'wiki_id': '20231101.simple_11278_4', 'url': 'https://simple.wikipedia.org/wiki/Book%20of%20Genesis'}\n",
      "{'title': 'Rock Demers', 'text': 'Rock Demers,  (December 11, 1933 – August 17, 2021) was a Canadian movie producer.  He was the founder of the movie company Les Productions la Fête and produced the Tales for All movie series for children. He also produced The Dog Who Stopped the War, The Peanut Butter Solution and Vincent and Me. Demers was born in Sainte-Cécile-de-Lévrard, Quebec.', 'wiki_id': '20231101.simple_864656_0', 'url': 'https://simple.wikipedia.org/wiki/Rock%20Demers'}\n",
      "{'title': 'Habesha peoples', 'text': 'Egyptian inscriptions refer to the people that they traded with in Punt as , \"the bearded ones.\" Francis Breyer believes the Egyptian demonym to be the source of the Semitic term.', 'wiki_id': '20231101.simple_839274_8', 'url': 'https://simple.wikipedia.org/wiki/Habesha%20peoples'}\n",
      "{'title': 'Brazil at the 2018 Winter Paralympics', 'text': 'The table below contains the list of members of people (called \"Team Brazil\") that will be participating in the 2018 Games.', 'wiki_id': '20231101.simple_620636_2', 'url': 'https://simple.wikipedia.org/wiki/Brazil%20at%20the%202018%20Winter%20Paralympics'}\n",
      "{'title': 'Satoshi Kukino', 'text': '|2006||rowspan=\"4\"|Kawasaki Frontale||rowspan=\"4\"|J. League 1||0||0||0||0||0||0||colspan=\"2\"|-||0||0', 'wiki_id': '20231101.simple_241540_0', 'url': 'https://simple.wikipedia.org/wiki/Satoshi%20Kukino'}\n",
      "{'title': 'Jurassic Coast', 'text': 'Pretty much the same rocks can be seen near the Yorkshire coast near Hull (Kingston upon Hull) in Yorkshire near the Humber Estuary. Also the Jurassic and Cretaceous rocks can be followed into northern France.', 'wiki_id': '20231101.simple_139320_7', 'url': 'https://simple.wikipedia.org/wiki/Jurassic%20Coast'}\n",
      "{'title': 'Outkast', 'text': \"On October 31, 2000 Stankonia was released. It entered the Billboard 200 at number two after selling over 530,000 copies in its first week. Outkast's first greatest hits album Big Boi and Dre Present... Outkast was released on December 4, 2001.\", 'wiki_id': '20231101.simple_431000_4', 'url': 'https://simple.wikipedia.org/wiki/Outkast'}\n",
      "{'title': 'Taliban insurgency', 'text': 'Areas where the security situation is worse produce more Opium; areas that are more stable seem to produce less. Many farmers in rural areas depend on selling poppy seeds. Opium is more profitable than wheat and destroying opium fields could possibly lead to discontent or unrest among the affected population.  Some 3.3 million Afghans are involved in producing opium. For this reason, some people say that eradicating  poppy crops is not a viable option. Some poppy eradication programs have, however, proven effective, especially in the north of Afghanistan. The  opium poppy eradication program of Balkh Governor Ustad Atta Mohammad Noor between 2005 and 2007 successfully reduced poppy cultivation in Balkh Province from 7,200\\xa0hectares in 2005 to zero by 2007.', 'wiki_id': '20231101.simple_586827_4', 'url': 'https://simple.wikipedia.org/wiki/Taliban%20insurgency'}\n",
      "{'title': 'Elmdale, Kansas', 'text': 'Elmdale is a city in Chase County, Kansas, United States. In 2010, 55 people lived there. It is along the U.S. Route 50 highway.', 'wiki_id': '20231101.simple_874694_0', 'url': 'https://simple.wikipedia.org/wiki/Elmdale%2C%20Kansas'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dt = prepare_dataset()\n",
    "\n",
    "counter = 10\n",
    "for item in tqdm(dt):\n",
    "    print(item)\n",
    "\n",
    "    counter -= 1\n",
    "    if(counter == 0): break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The import function\n",
    "\n",
    "`TODO:`\n",
    "* add a function to add objects to batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "def import_wiki_data(max_rows=100_000):\n",
    "    print(f\"Importing {max_rows} data items\")\n",
    "\n",
    "    dataset = prepare_dataset()\n",
    "    wiki = client.collections.get(\"Wiki\")\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    with wiki.batch.fixed_size(batch_size=2500, concurrent_requests=4) as batch:\n",
    "        for item in tqdm(dataset, total=max_rows):\n",
    "\n",
    "            data_to_insert = {   \n",
    "                \"wiki_id\": item[\"wiki_id\"],\n",
    "                \"text\": item[\"text\"],\n",
    "                \"title\": item[\"title\"],\n",
    "                \"url\": item[\"url\"],\n",
    "            }\n",
    "\n",
    "            item_id = generate_uuid5(item[\"wiki_id\"])\n",
    "\n",
    "            # vector = item[\"vector\"]\n",
    "            item_vector = {\n",
    "                \"main_vector\": item[\"vector\"]\n",
    "            }\n",
    "\n",
    "            batch.add_object(\n",
    "                properties=data_to_insert,\n",
    "                \n",
    "                uuid=item_id,\n",
    "                vector=item_vector\n",
    "            )\n",
    "\n",
    "            # Check number of errors while running\n",
    "            if(batch.number_errors > 10):\n",
    "                print(f\"Reached {batch.number_errors} Errors during batch import\")\n",
    "                break\n",
    "            \n",
    "            # stop after the request number reaches = max_rows\n",
    "            counter += 1\n",
    "            if(counter >= max_rows):\n",
    "                break\n",
    "    \n",
    "    # check for errors at the end\n",
    "    if (len(wiki.batch.failed_objects)>0):\n",
    "        print(\"Final error check\")\n",
    "        print(f\"Some errors {len(wiki.batch.failed_objects)}\")\n",
    "        print(wiki.batch.failed_objects[-1])\n",
    "    \n",
    "    print(f\"Imported {counter} items\")\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_wiki_data(100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if data loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = client.collections.get(\"Wiki\")\n",
    "len(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wiki.query.fetch_objects(limit=1, include_vector=True)\n",
    "print(res.objects[0].properties)\n",
    "print(res.objects[0].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
