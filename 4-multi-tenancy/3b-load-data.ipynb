{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-tenant Chat with Papers - Load and chunk papers\n",
    "## Get keys and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate Key:root-user-key\n",
      "OpenAI API Key: sk-dummy-key-for-local-testing\n",
      "OpenAI URL: http://host.docker.internal:11434\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WEAVIATE_KEY = os.getenv(\"WEAVIATE_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_URL = os.getenv(\"OPENAI_URL\")\n",
    "\n",
    "print(f\"Weaviate Key:{WEAVIATE_KEY}\")\n",
    "print(f\"OpenAI API Key: {OPENAI_API_KEY}\")\n",
    "print(f\"OpenAI URL: {OPENAI_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-19 00:22:51,755] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-19 00:22:51,840] INFO in _client: HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:51,863] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/.well-known/ready \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Connect to the local instance\n",
    "client = weaviate.connect_to_local(\n",
    "  host=\"127.0.0.1\", # the address to the learner's instance\n",
    "  port=8080,\n",
    "  grpc_port=50051,\n",
    "  auth_credentials=Auth.api_key(WEAVIATE_KEY),\n",
    "  headers={\n",
    "    \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
    "  }\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from arxiv\n",
    "\n",
    "1. Get chunks from paper - `get_chunks_from_paper`\n",
    "2. Create a tenant for the paper - `create_tenant`\n",
    "3. Batch import chunks - `batch_import_chunks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get chunks from paper - `get_chunks_from_paper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distyll.text import from_arxiv_paper\n",
    "from distyll.utils import chunk_text\n",
    "\n",
    "def get_chunks_from_paper(url):\n",
    "    paper = from_arxiv_paper(url)\n",
    "    chunks = chunk_text(source_text=paper[\"text\"])\n",
    "\n",
    "    paper[\"arxiv_id\"] = url.replace(\"https://arxiv.org/pdf/\", \"\").replace(\".pdf\", \"\").replace(\".\", \"-\")\n",
    "    paper[\"chunks\"] = chunks\n",
    "    return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-19 00:22:51,882] INFO in text: Getting arXiV paper from https://arxiv.org/pdf/2212.10496.pdf\n",
      "[2025-09-19 00:22:51,887] INFO in utils: Getting arXiV title from https://arxiv.org/abs/2212.10496\n",
      "[2025-09-19 00:22:51,975] INFO in utils: Chunking text of 41145 characters with words method.\n",
      "[2025-09-19 00:22:51,979] INFO in utils: Chunking text of 41125 chars by number of words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Precise Zero-Shot Dense Retrieval without Relevance Labels',\n",
       " 'url': 'https://arxiv.org/pdf/2212.10496.pdf',\n",
       " 'text': '\\nPrecise Zero-Shot Dense Retrieval without Relevance Labels\\nLuyu Gao∗†Xueguang Ma∗‡Jimmy Lin‡Jamie Callan†\\n†Language Technologies Institute, Carnegie Mellon University\\n‡David R. Cheriton School of Computer Science, University of Waterloo\\n{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca\\nAbstract\\nWhile dense retrieval has been shown effec-\\ntive and efﬁcient across tasks and languages,\\nit remains difﬁcult to create effective fully\\nzero-shot dense retrieval systems when no rel-\\nevance label is available. In this paper, we\\nrecognize the difﬁculty of zero-shot learning\\nand encoding relevance. Instead, we pro-\\npose to pivot through Hy pothetical D ocument\\nEmbeddings ( HyDE ). Given a query, HyDE ﬁrst\\nzero-shot instructs an instruction-following\\nlanguage model (e.g. InstructGPT ) to gen-\\nerate a hypothetical document. The docu-\\nment captures relevance patterns but is unreal\\nand may contain false details. Then, an un-\\nsupervised contrastively learned encoder (e.g.\\nContriever ) encodes the document into an\\nembedding vector. This vector identiﬁes a\\nneighborhood in the corpus embedding space,\\nwhere similar real documents are retrieved\\nbased on vector similarity. This second step\\nground the generated document to the actual\\ncorpus, with the encoder’s dense bottleneck\\nﬁltering out the incorrect details. Our exper-\\niments show that HyDE signiﬁcantly outper-\\nforms the state-of-the-art unsupervised dense\\nretriever Contriever and shows strong per-\\nformance comparable to ﬁne-tuned retrievers,\\nacross various tasks (e.g. web search, QA, fact\\nveriﬁcation) and languages (e.g. sw, ko, ja).1\\n1 Introduction\\nDense retrieval (Lee et al., 2019; Karpukhin et al.,\\n2020), the method of retrieving documents using\\nsemantic embedding similarities, has been shown\\nsuccessful across tasks like web search, question\\nanswering, and fact veriﬁcation. A variety of meth-\\nods such as negative mining (Xiong et al., 2021; Qu\\net al., 2021), distillation (Qu et al., 2021; Lin et al.,\\n2021b; Hofstätter et al., 2021) and task-speciﬁc\\n∗Equal contribution.\\n1No models were trained or ﬁne-tuned in making this pre-\\nprint. Our open source code is available at https://github.\\ncom/texttron/hyde .pre-training (Izacard et al., 2021; Gao and Callan,\\n2021; Lu et al., 2021; Gao and Callan, 2022; Liu\\nand Shao, 2022) have been proposed to improve the\\neffectiveness of supervised dense retrieval models.\\nOn the other hand, zero-shot dense retrieval still\\nremains difﬁcult. Many recent works consider the\\nalternative transfer learning setup, where the dense\\nretrievers are trained on a high-resource dataset and\\nthen evaluated on queries from new tasks. The MS-\\nMARCO collection (Bajaj et al., 2016), a massive\\njudged dataset with a large number of judged query-\\ndocument pairs, is arguably the most commonly\\nused. As argued by Izacard et al. (2021), in prac-\\ntice, however, the existence of such a large dataset\\ncannot always be assumed. Even MS-MARCO re-\\nstricts commercial use and cannot be adopted in a\\nvariety of real-world search scenarios.\\nIn this paper, we aim to build effective fully\\nzero-shot dense retrieval systems that require no\\nrelevance supervision, work out-of-box and gener-\\nalize across tasks. As supervision is not available,\\nwe start by examining self-supervised representa-\\ntion learning methods. Modern deep learning en-\\nables two distinct learning algorithms. At the token\\nlevel, generative large language models (LLM) pre-\\ntrained on large corpus have demonstrated strong\\nnatural language understanding (NLU) and gen-\\neration (NLG) capabilities (Brown et al., 2020;\\nChen et al., 2021; Rae et al., 2021; Hoffmann\\net al., 2022; Thoppilan et al., 2022; Chowdhery\\net al., 2022). At the document level, text (chunk)\\nencoders pre-trained with contrastive objectives\\nlearn to encode document-document similarity into\\ninner-product (Izacard et al., 2021; Gao and Callan,\\n2022). On top of these, one extra insight into LLM\\nis borrowed: the LLMs further trained to follow\\ninstructions can zero-shot generalize to diverse un-\\nseen instructions (Ouyang et al., 2022; Sanh et al.,\\n2022; Min et al., 2022; Wei et al., 2022). Ouyang\\net al. (2022) show that with a small amount of data,\\nGPT-3 (Brown et al., 2020) models can be alignedarXiv:2212.10496v1  [cs.IR]  20 Dec 2022\\nHyDE\\nGPTContrieverhow long does it take to remove\\nwisdom tooth It usually takes between 30\\nminutes and two hours to\\nremove a wisdom tooth...How wisdom teeth are removed... \\nSome ... a few minutes, whereas\\nothers can take 20 minutes or\\nlonger ....\\nHow has the COVID-19 pandemic impacted\\nmental health?...depression and anxiety had\\nincreased by 20% since the\\nstart of the pandemic...... two studies investigating\\nCOVID-19 patients ... significantly\\nhigher level of depressive ...write a passage to answer the question\\nwrite a scientific paper passage to answer\\nthe question\\n인간은  언제  불을  사용했는가 ?write a passage in Korean to answer the\\nquestion in detail인간이  불을  사용한  기록은  약\\n800 만년  전부터  나타난다 ...... 불을  처음  사용한  시기는  호모\\n에렉투스가  살았던  142 만  년  전으\\n로 거슬러간다 ...\\ninstruction query generated document real documentFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries\\nwithout changing the underlying GPT-3 andContriever /mContriever models.\\nto human intent to follow instructions.\\nWith these ingredients, we propose to\\npivot through Hypothetical Document\\nEmbeddings ( HyDE ), and decompose dense\\nretrieval into two tasks, a generative task per-\\nformed by an instruction-following language\\nmodel and a document-document similarity task\\nperformed by a contrastive encoder (Figure 1).\\nFirst, we feed the query to the generative model\\nand instruct it to \"write a document that answers\\nthe question\", i.e. a hypothetical document.\\nWe expect the generative process to capture\\n\"relevance\" by giving an example; the generated\\ndocument is not real, can contain factual errors but\\nis like a relevant document. In the second step,\\nwe use an unsupervised contrastive encoder to\\nencode this document into an embedding vector.\\nHere, we expect the encoder’s dense bottleneck\\nto serve a lossy compressor, where the extra\\n(hallucinated) details are ﬁltered out from the\\nembedding. We use this vector to search against\\nthe corpus embeddings. The most similar real\\ndocuments are retrieved and returned. The retrieval\\nleverages document-document similarity encoded\\nin the inner-product during contrastive training.\\nNote that, interestingly, with HyDE factorization,\\nthe query-document similarity score is no longer\\nexplicitly modeled nor computed. Instead, the\\nretrieval task is cast into two NLU and NLG tasks.\\nHyDE appears unsupervised. Nomodel is trained\\ninHyDE : both the generative model and the con-\\ntrastive encoder remain intact. Supervision signals\\nwere only involved in instruction learning of our\\nbackbone LLM.\\nIn our experiments, we show HyDE using Instruct-\\nGPT (Ouyang et al., 2022) and Contriever (Izacard\\net al., 2021) as backbone models signiﬁcantly out-\\nperforms the previous state-of-the-art Contriever-\\nonly zero-shot no-relevance system on 11 queriessets, covering tasks like Web Search, Question\\nAnswering, Fact Veriﬁcation and languages like\\nSwahili, Korean, Japanese.\\n2 Related Works\\nDense Retrieval (Lee et al., 2019; Karpukhin\\net al., 2020) has been extensively studied after the\\nemergence of pre-trained Transformer language\\nmodels (Devlin et al., 2019). Researchers stud-\\nied the metric learning problems, such as training\\nloss (Karpukhin et al., 2020) and negative sam-\\npling (Xiong et al., 2021; Qu et al., 2021), and also\\nintroduced distillation (Qu et al., 2021; Lin et al.,\\n2021b; Hofstätter et al., 2021). Later works studied\\nthe second stage pre-training of language model\\nspeciﬁcally for retrieval (Izacard et al., 2021; Gao\\nand Callan, 2021; Lu et al., 2021; Gao and Callan,\\n2022; Liu and Shao, 2022).\\nThe popularity of dense retrieval can be partially\\nattributed to the rich and successful research in very\\nefﬁcient minimum inner product search (MIPS) at\\nvery large (billion) scales (Johnson et al., 2017).\\nInstructions-Following Language Models\\nSoon after the emergence of LLMs, several groups\\nof researchers discover that LLMs trained on data\\nconsisting of instructions and their execution can\\nzero-shot generalize to perform new tasks with new\\ninstructions (Ouyang et al., 2022; Sanh et al., 2022;\\nMin et al., 2022; Wei et al., 2022). This can be\\ndone by standard supervised sequence-to-sequence\\nlearning or more effectively with reinforcement\\nlearning (Ouyang et al., 2022).\\nConcurrent to us, Asai et al. (2022) studied\\n“Task-aware Retrieval with Instructions”. They\\nﬁne-tuned dense encoders that can also encode\\ntask-speciﬁc instruction prepended to query. In\\ncomparison, we use an unsupervised encoder and\\nhandle different tasks and their instruction with an\\ninstruction following generative LLM, as described\\nabove.\\nZero-Shot Dense Retrieval The tasks of zero-\\nshot (dense) retrieval are arguably empirically de-\\nﬁned by Thakur et al. (2021) for the neural re-\\ntrieval community. Their BEIR benchmark con-\\nsists of diverse retrieval tasks. The paper and\\nmany follow-up research generally consider the\\nTransfer Learning setup where the dense re-\\ntriever is ﬁrst learned using a diverse and richly\\nsupervised corpus and query collection, namely\\nMS-MARCO (Thakur et al., 2021; Wang et al.,\\n2022; Yu et al., 2022).\\nHowever, as stated by Izacard et al. (2021), such\\na large collection can rarely be assumed. In this\\npaper, therefore, we study the problem of building\\neffective dense retrieval systems without relevance\\nlabels. Similar to Izacard et al. (2021), we also\\ndo not assume access to the test time corpora for\\ntraining. This is a more realistic setup and prevents\\nover-engineering on the test corpora.\\nBy the deﬁnition in Sachan et al. (2022), our\\nsetup can be roughly considered as “unsuper-\\nvised” . Strictly, as with Sachan et al. (2022), the\\nonly supervision resides in the LLM, in the pro-\\ncessing of learning to follow instructions.\\nGenerative Retrieval Generative search is a new\\nclass of retrieval methods that use neural generative\\nmodels as search indices (Metzler et al., 2021; Tay\\net al., 2022; Bevilacqua et al., 2022; Lee et al.,\\n2022). These models use (constrained) decoding\\nto generate document identiﬁers, such as id and\\nsub-string, which map directly to realdocuments.\\nThey have to go through special training procedures\\nover relevance data; effective search may also need\\nto use novel forms of search indices (Bevilacqua\\net al., 2022; Lee et al., 2022). In comparison, our\\nmethod uses the standard MIPS index and requires\\nno training or training data. Our generative model\\nproduces an intermediate hypothetical document\\nto be fed into a dense encoder, instead of a real\\ndocument.\\n3 Methodology\\nIn this section, we ﬁrst formally deﬁne the prob-\\nlem of (zero-shot) dense retrieval. Then we will\\nintroduce how HyDE is designed to solve it.3.1 Preliminaries\\nDense retrieval models similarity between query\\nand document with inner product similarity. Given\\na query qand document d, it uses two encoder\\nfunction encqandencdto map them into ddimen-\\nsion vectors vq,vd, whose inner product is used\\nas similarity measurement.\\nsim(q,d) =⟨encq(q),encd(d)⟩=⟨vq,vd⟩(1)\\nFor zero-shot retrieval, we consider Lquery sets\\nQ1, Q2, ..., Q Land their corresponding search cor-\\npus, document sets D1, D2, ..., D L. Denote the\\nj-th query from i-th set query set Qiasqij. We\\nneed to fully deﬁne mapping functions encqand\\nencdwithout access to any query set Qi, document\\nsetDi, or any relevance judgment rij.\\nThe difﬁculty of zero-shot dense retrieval lies\\nprecisely in Equation 1: it requires learning of two\\nembedding functions (for query and document re-\\nspectively) into the same embedding space where\\ninner product captures relevance . Without rele-\\nvance judgments/scores to ﬁt, learning becomes\\nintractable.\\n3.2 HyDE\\nHyDE circumvents the aforementioned learning\\nproblem by performing search in document-\\nonly embedding space that captures document-\\ndocument similarity. This can be easily learned\\nusing unsupervised contrastive learning (Izacard\\net al., 2021; Gao et al., 2021; Gao and Callan,\\n2022). We set document encoder encddirectly as a\\ncontrastive encoder enc con.\\nf=encd=enc con (2)\\nThis function is also denoted as ffor simplic-\\nity. This unsupervised contrastive encoder will\\nbe shared by all incoming document corpus.\\nvd=f(d)∀d∈D1∪D2∪...∪DL (3)\\nTo build the query vector, we consider in addition\\nan instruction following LM, InstructLM. It takes a\\nquery qand a textual instruction INST and follows\\nthem to perform the task speciﬁed by INST . For\\nsimplicity, denote,\\ng(q,INST) =InstructLM (q,INST) (4)\\nNow we can use gto map queries to \"hypotheti-\\ncal\" documents by sampling from g, setting INST\\nto be“write a paragraph that answers the\\nquestion” . The generated document is not real,\\ncan and is likely to be ungrounded factually (Brown\\net al., 2020; Thoppilan et al., 2022). We only re-\\nquire it to capture relevance pattern. This is done\\nby generating documents, i.e. providing exam-\\nples. Critically, here we ofﬂoad relevance mod-\\neling from representation learning model to an\\nNLG model that generalizes signiﬁcantly more eas-\\nily, naturally, and effectively (Brown et al., 2020;\\nOuyang et al., 2022). Generating examples also\\nreplaces explicit modeling of relevance scores.\\nWe can now encode the generated document using\\nthe document encoder f. Write,\\nE[vqij] =E[f(g(qij,INST i))] (5)\\nFormally, gdeﬁnes a probability distribution based\\non the chain rule. In this paper, we simply consider\\nthe expectation value, assuming the distribution of\\nvqijis uni-modal, i.e. the query is not ambiguous.\\nThe study of ambiguous queries and diversity is\\nleft to future work. We estimate Equation 5 by\\nsampling Ndocuments from g,[ˆd1,ˆd2, ...,ˆdN].\\nˆvqij=1\\nN∑\\nˆdk∼g(qij,INST i)f(dk) (6)\\n=1\\nNN∑\\nk=1f(ˆdk) (7)\\nWe also consider the query as a possible hypothesis,\\nˆvqij=1\\nN+ 1[N∑\\nk=1f(ˆdk) +f(qij)] (8)\\nInner product is computed between ˆvqijand the\\nset of all document vectors {f(d)|d∈Di}. The\\nmost similar documents are retrieved. Here the\\nencoder function fserves as a lossy compressor\\nthat outputs dense vectors, where the extra details\\nare ﬁltered and left out from the vector. It further\\ngrounds the hypothetical vector to the actual corpus\\nand the real documents. The full HyDE system is\\nillustrated in Figure 1.\\n4 Experiments\\n4.1 Setup\\nImplementation We implement HyDE using\\nInstructGPT , a GPT-3 model from the instruct\\nseries (text-davinci-003 ; Ouyang et al. (2022))\\nandContriever models (Izacard et al., 2021). Wesample from InstructGPT using the OpenAI play-\\nground default temperature of 0.7 for open-ended\\ngenerations. We use the English-only Contriever\\nmodel for English retrieval tasks and multilingual\\nmContriever for non-English tasks. We conducted\\nretrieval experiments with the Pyserini toolkit (Lin\\net al., 2021a).\\nDatasets We consider web search query sets\\nTREC DL19 (Craswell et al., 2020a) and\\nDL20 (Craswell et al., 2020b); they are based on\\nthe MS-MARCO dataset (Bajaj et al., 2016). We\\nalso use a diverse collection of 6 low-resource\\ndatasets from the BEIR dataset (Thakur et al.,\\n2021). For non-English retrieval, we consider\\nSwahili, Korean, Japanese, and Bengali from the\\nMr.Tydi dataset (Zhang et al., 2021).\\nWe use different instructions for each dataset.\\nThey share a similar structure but have different\\nquantiﬁers to control the exact form of the gener-\\nated hypothetical documents. These instructions\\ncan be found in subsection A.1.\\nCompared Systems Contriever models,\\nContriever andmContriever , serve as our major\\nbaseline. They are trained using unsupervised\\ncontrastive learning. HyDE retrievers share the\\nexact same embedding spaces with them. The\\nonly difference is how the query vector is built.\\nThese comparisons allow us to easily examine\\nthe effect of HyDE . The classical heuristic-based\\nlexical retriever BM25 is also included.\\nSeveral systems that involve ﬁne-tuning on mas-\\nsive relevance data are also included as refer-\\nences. We consider models ﬁne-tuned on MS-\\nMARCO and transferred, DPR and ANCE, from\\nthe BEIR paper. For multilingual, we include\\nthe mDPR model from Mr.Tydi paper and MS-\\nMARCO ﬁne-tuned mBERT and XLM-R from\\nthe Contriever paper. We also include the state-of-\\nthe-art transfer learning models: Contriever and\\nmContriever ﬁne-tuned on MS-MARCO, denoted\\nContrieverFTandmContrieverFT. These mod-\\nels have run through the state-of-the-art retrieval\\nmodel training pipeline that involves second-stage\\nretrieval-speciﬁc pre-training (Lee et al., 2019) and\\na few rounds of ﬁne-tuning (Qu et al., 2021); they\\nshould be considered empirical upper bounds.\\n4.2 Web Search\\nIn Table 1, we show retrieval results on TREC\\nDL19 and TREC DL20. We see HyDE bring sizable\\nimprovements to Contriever across the board for\\nDL19 DL20\\nmap ndcg@10 recall@1k map ndcg@10 recall@1k\\nw/o relevance judgement\\nBM25 30.1 50.6 75.0 28.6 48.0 78.6\\nContriever 24.0 44.5 74.6 24.0 42.1 75.4\\nHyDE 41.8 61.3 88.0 38.2 57.9 84.4\\nw/ relevance judgement\\nDPR 36.5 62.2 76.9 41.8 65.3 81.4\\nANCE 37.1 64.5 75.5 40.8 64.6 77.6\\nContrieverFT41.7 62.1 83.6 43.6 63.2 85.8\\nTable 1: Results for web search on DL19/20. Best performing w/o relevance and overall system(s) are marked\\nbold . DPR, ANCE and ContrieverFTare in-domain supervised models that are ﬁnetuned on MS MARCO training\\ndata.\\nScifact Arguana Trec-Covid FiQA DBPedia TREC-NEWS\\nnDCG@10\\nw/o relevance judgement\\nBM25 67.9 39.7 59.5 23.6 31.8 39.5\\nContriever 64.9 37.9 27.3 24.5 29.2 34.8\\nHyDE 69.1 46.6 59.3 27.3 36.8 44.0\\nw/ relevance judgement\\nDPR 31.8 17.5 33.2 29.5 26.3 16.1\\nANCE 50.7 41.5 65.4 30.0 28.1 38.2\\nContrieverFT67.7 44.6 59.6 32.9 41.3 42.8\\nRecall@100\\nw/o relevance judgement\\nBM25 92.5 93.2 49.8 54.0 46.8 44.7\\nContriever 92.6 90.1 17.2 56.2 45.3 42.3\\nHyDE 96.4 97.9 41.4 62.1 47.2 50.9\\nw/ relevance judgement\\nDPR 72.7 75.1 21.2 34.2 34.9 21.5\\nANCE 81.6 93.7 45.7 58.1 31.9 39.8\\nContrieverFT94.7 97.7 40.7 65.6 54.1 49.2\\nTable 2: Low resource tasks from BEIR. Best performing w/o relevance and overall system(s) are marked bold .\\nboth precision-oriented and recall metrics. While\\nunsupervised Contriever can underperform the\\nclassical BM25 approach, HyDE outperforms BM25\\nby large margins.\\nHyDE remains competitive even when compared\\nto ﬁne-tuned models. Note that TREC DL19/20\\nare search tasks deﬁned on MS-MARCO and\\nthere, all the ﬁne-tuned models are richly super-\\nvised . On TREC DL19, HyDE shows comparable\\nmap and ndcg@10 to ContrieverFTand best re-\\ncall@1k. On DL20, HyDE gets around 10% lower\\nmap and ndcg@10 than ContrieverFTand sim-\\nilar recall@1k. The ANCE model shows better\\nndcg@10 numbers than HyDE but lower recall, sug-\\ngesting it may be biased to a subset of queries\\nand/or relevant documents.4.3 Low Resource Retrieval\\nIn Table 2, we show retrieval results on low-\\nresource tasks from BEIR. Similar to web\\nsearch,HyDE again brings sizable improvements to\\nContriever across the board in terms of both ndcg\\nand recall. HyDE is only outperformed by BM25 on\\none dataset, TREC-Covid but with a tiny 0.2 mar-\\ngin; in comparison, the underlying Contriever\\nunderperforms by more than 50%.\\nWe also observe HyDE demonstrates strong\\nperformance compared to ﬁne-tuned models.\\nHyDE generally shows better performance than\\nANCE and DPR, even though the two are\\nﬁne-tuned on MS-MARCO and ANCE also in-\\nvolves some sophisticated hard negative techniques.\\nContrieverFTshows performance advantages on\\nFiQA and DBPedia. These involve retrieval of ﬁ-\\nnancial posts or entities respectively. We believe\\nthe performance difference can be attributed to the\\nSwahili Korean Japanese Bengali\\nw/o relevance judgement\\nBM25 38.9 28.5 21.2 41.8\\nmContriever 38.3 22.3 19.5 35.3\\nHyDE 41.7 30.6 30.7 41.3\\nw/ relevance judgement\\nmDPR 7.3 21.9 18.1 25.8\\nmBERT 37.4 28.1 27.1 35.1\\nXLM-R 35.1 32.2 24.8 41.7\\nmContrieverFT51.2 34.2 32.4 42.3\\nTable 3: MRR@100 on Mr.Tydi. Best performing w/o\\nrelevance and overall system(s) are marked bold .\\nunder-speciﬁcation of the instruction; more elabo-\\nrative instructions may help.\\n4.4 Multilingual Retrieval\\nMultilingual setup poses several additional chal-\\nlenges to HyDE . The small-sized contrastive en-\\ncoder gets saturated as the number of languages\\nscales (Conneau et al., 2020; Izacard et al., 2021).\\nMeanwhile, our generative LLM faces an opposite\\nissue: with languages of not as high resource as\\nEnglish or French, the high capacity LLM can get\\nunder-trained (Hoffmann et al., 2022).\\nNevertheless, in Table 3, we still ﬁnd HyDE\\nable to improve the mContriever model. It can\\noutperform non-Contriever models ﬁne-tuned on\\nand transferred from MS-MARCO. On the other\\nhand, we do observe some margins between HyDE\\nand ﬁne-tuned mContrieverFT. SinceHyDE and\\nmContrieverFTuse similar contrastive encoders,\\nwe hypothesize this is because the non-English lan-\\nguages we considered are under-trained in both\\npre-training and instruction learning stages.\\n5 Analysis\\nThe generative LLM and contrastive encoder make\\nup the backbone of HyDE . In this section, we study\\nthe effect of changing their realizations. In partic-\\nular, we consider smaller language models (LM)\\nand ﬁne-tuned encoders. We conduct our studies\\non TREC DL19/20.\\n5.1 Effect of Different Generative Models\\nIn Table 4, we show HyDE using other\\ninstruction-following language models. In\\nparticular, we consider a 52-billion Cohere\\nmodel ( command-xlarge-20221108 ) and a\\n11-billion FLAN model ( FLAN-T5-xxl ; Wei\\net al. (2022)).2Generally, we observe that all\\n2Model sizes are from https://crfm.stanford.edu/\\nhelm/v1.0/?models .Model DL19 DL20\\nContriever 44.5 42.1\\nContrieverFT62.1 63.2\\nHyDE\\nw/ Contriever\\nw/ Flan-T5 (11b) 48.9 52.9\\nw/ Cohere (52b) 53.8 53.8\\nw/ GPT (175b) 61.3 57.9\\nw/ ContrieverFT\\nw/ Flan-T5 (11b) 60.2 62.1\\nw/ Cohere (52b) 61.4 63.1\\nw/ GPT (175b) 67.4 63.5\\nTable 4: NDCG@10 on TREC DL19/20. Effect\\nof changing different instruction LMs and using ﬁne-\\ntuned encoder. Best w/o relevance and overall models\\nare marked bold .\\nmodels bring improvement to the unsupervised\\nContriever , with larger models bringing larger\\nimprovements. At the time when this paper is\\nwritten, the Cohere model is still experimental\\nwithout much detail disclosed. We can only\\ntentatively hypothesize that training techniques\\nmay have also played some role in the performance\\ndifference.\\n5.2 HyDE with Fine-tuned Encoder\\nTo begin with, HyDE with ﬁne-tuned encoder is\\nnotthe intended usage: HyDE is more powerful\\nand irreplaceable when few relevance labels are\\npresent. Here we are interested to ﬁnd out if\\nand howHyDE embedding can affect ﬁne-tuned en-\\ncoders. In Table 4, we see that less powerful instruc-\\ntion LMs can negatively impact the overall perfor-\\nmance of the ﬁne-tuned retriever. (To remind our\\nreaders,ContrieverFTis in-domain supervisedly\\nﬁne-tuned for TREC DL19/20). The performance\\ndegradations remain small. On the other hand, we\\nalso observe the InstructGPT model able to fur-\\nther bring up the performance, especially on DL19.\\nThis suggests that there may still exist certain fac-\\ntors not captured by the ﬁne-tuned encoder but only\\nby the generative model.\\n6 Conclusion\\nAt the end of the paper, we encourage the readers\\nto take a moment and reﬂect on the HyDE model.\\nCompare it to some of the other recently seen re-\\ntrievers or re-ranker. These other models probably\\ndiffer in their architecture, training method, and/or\\ntask, but probably all of them involve modeling\\nrelevance scores between a pair of query and docu-\\nment. Dense retrievers consider vector similarities\\nwhile self-attentive re-rankers regression scores. In\\ncomparison, the concept of relevance in HyDE is\\ncaptured by an NLG model and the language gener-\\nation process. We demonstrate in many cases, HyDE\\ncan be as effective as dense retrievers that learn to\\nmodel numerical relevance scores. So, is numeri-\\ncal relevance just a statistical artifact of language\\nunderstanding? Will a weak retriever theoretically\\nsufﬁce as the NLU & NLG models rapidly become\\nstronger? Rushing to conclusions is not smart;\\nmore works need to be done to get answers. With\\nthis paper, we just want to raise these questions.\\nConcretely in this paper, we introduce a new\\nparadigm of interactions between LLM and dense\\nencoder/retriever. We demonstrate (part of) rel-\\nevance modeling and instruction understanding\\ncan be delegated to the more powerful and ﬂex-\\nible LLM. As a consequence, the need for rele-\\nvance labels is removed. We are excited to see\\nhow this can be generalized further to more so-\\nphisticated tasks like multi-hop retrieval/QA and\\nconversational search.\\nWe argue HyDE is also of practical use though not\\nnecessarily over the entire lifespan of a search sys-\\ntem. At the very beginning of the life of the search\\nsystem, serving queries using HyDE offers perfor-\\nmance comparable to a ﬁne-tuned model, which\\nno other relevance-free model can offer. As the\\nsearch log grows, a supervised dense retriever can\\nbe gradually rolled out. As the dense retriever\\ngrows stronger, more queries will be routed to it,\\nwith only less common and emerging ones going\\ntoHyDE backend.\\nReferences\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\\njishirzi, and Wen-tau Yih. 2022. Task-aware re-\\ntrieval with instructions.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\\nwary, and Tong Wang. 2016. Ms marco: A human\\ngenerated machine reading comprehension dataset.\\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick\\nLewis, Wen-tau Yih, Sebastian Riedel, and Fabio\\nPetroni. 2022. Autoregressive search engines: Gen-\\nerating substrings as document identiﬁers. CoRR ,\\nabs/2204.10628.Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen,\\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam Mc-\\nCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. 2020. Language models are few-shot learn-\\ners. In Advances in Neural Information Processing\\nSystems 33: Annual Conference on Neural Informa-\\ntion Processing Systems 2020, NeurIPS 2020, De-\\ncember 6-12, 2020, virtual .\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\\nKaiser, Mohammad Bavarian, Clemens Winter,\\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\\nWilliam Saunders, Christopher Hesse, Andrew N.\\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\\nMorikawa, Alec Radford, Matthew Knight, Miles\\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\\nIlya Sutskever, and Wojciech Zaremba. 2021. Eval-\\nuating large language models trained on code.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\\ndus, Denny Zhou, Daphne Ippolito, David Luan,\\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\\nErica Moreira, Rewon Child, Oleksandr Polozov,\\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\\nPalm: Scaling language modeling with pathways.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\\nVishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\\ncross-lingual representation learning at scale. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 8440–\\n8451, Online. Association for Computational Lin-\\nguistics.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M. V oorhees. 2020a. Overview\\nof the trec 2019 deep learning track.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz,\\nDaniel Fernando Campos, and Ellen M. V oorhees.\\n2020b. Overview of the trec 2020 deep learning\\ntrack. ArXiv , abs/2003.07820.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\\ntraining architecture for dense retrieval. In Proceed-\\nings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing , pages 981–993,\\nOnline and Punta Cana, Dominican Republic. Asso-\\nciation for Computational Linguistics.\\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\\npus aware language model pre-training for dense\\npassage retrieval. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers) , pages 2843–2853,\\nDublin, Ireland. Association for Computational Lin-\\nguistics.\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\\nSimCSE: Simple contrastive learning of sentence\\nembeddings. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 6894–6910, Online and Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\\nKatie Millican, George van den Driessche, Bogdan\\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\\nand Laurent Sifre. 2022. Training compute-optimal\\nlarge language models.\\nSebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\\nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\\nﬁciently teaching an effective dense retriever with\\nbalanced topic aware sampling. In Proceedings of\\nthe 44th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval ,SIGIR ’21, page 113–122, New York, NY , USA. As-\\nsociation for Computing Machinery.\\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\\nand Edouard Grave. 2021. Towards unsupervised\\ndense information retrieval with contrastive learning.\\nCoRR , abs/2112.09118.\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\\nBillion-scale similarity search with gpus. CoRR ,\\nabs/1702.08734.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781, Online. Association for Computational Lin-\\nguistics.\\nHyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon\\nSeo. 2022. Generative multi-hop retrieval.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\\n2021a. Pyserini: A Python toolkit for reproducible\\ninformation retrieval research with sparse and dense\\nrepresentations. In Proceedings of the 44th Annual\\nInternational ACM SIGIR Conference on Research\\nand Development in Information Retrieval (SIGIR\\n2021) , pages 2356–2362.\\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\\n2021b. In-batch negatives for knowledge distillation\\nwith tightly-coupled teachers for dense retrieval. In\\nProceedings of the 6th Workshop on Representation\\nLearning for NLP (RepL4NLP-2021) , pages 163–\\n173, Online. Association for Computational Linguis-\\ntics.\\nZheng Liu and Yingxia Shao. 2022. Retromae: Pre-\\ntraining retrieval-oriented transformers via masked\\nauto-encoder. ArXiv , abs/2205.12035.\\nShuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed\\nMalik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu,\\nand Arnold Overwijk. 2021. Less is more: Pre-\\ntrain a strong Siamese encoder for dense text re-\\ntrieval using a weak decoder. In Proceedings of the\\n2021 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 2780–2791, Online and\\nPunta Cana, Dominican Republic. Association for\\nComputational Linguistics.\\nDonald Metzler, Yi Tay, Dara Bahri, and Marc Najork.\\n2021. Rethinking search: making domain experts\\nout of dilettantes. SIGIR Forum , 55(1):13:1–13:27.\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\\nin context. In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies , pages 2791–2809, Seattle, United States.\\nAssociation for Computational Linguistics.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\\nCarroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex\\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\\nLuke Miller, Maddie Simens, Amanda Askell, Pe-\\nter Welinder, Paul Christiano, Jan Leike, and Ryan\\nLowe. 2022. Training language models to follow in-\\nstructions with human feedback.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\\nand Haifeng Wang. 2021. RocketQA: An opti-\\nmized training approach to dense passage retrieval\\nfor open-domain question answering. In Proceed-\\nings of the 2021 Conference of the North Ameri-\\ncan Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n5835–5847, Online. Association for Computational\\nLinguistics.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\\nMillican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susan-\\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\\ncob Menick, Albin Cassirer, Richard Powell, George\\nvan den Driessche, Lisa Anne Hendricks, Mari-\\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\\nJonathan Uesato, John Mellor, Irina Higgins, An-\\ntonia Creswell, Nat McAleese, Amy Wu, Erich\\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\\nDavid Budden, Esme Sutherland, Karen Simonyan,\\nMichela Paganini, Laurent Sifre, Lena Martens,\\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\\nmatzadeh, Elena Gribovskaya, Domenic Donato,\\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\\nprien de Masson d’Autume, Yujia Li, Tayfun Terzi,\\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\\nDiego de Las Casas, Aurelia Guy, Chris Jones,\\nJames Bradbury, Matthew Johnson, Blake Hecht-\\nman, Laura Weidinger, Iason Gabriel, William Isaac,\\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\\nway, Lorrayne Bennett, Demis Hassabis, Koray\\nKavukcuoglu, and Geoffrey Irving. 2021. Scal-\\ning language models: Methods, analysis & insights\\nfrom training gopher.\\nDevendra Singh Sachan, Mike Lewis, Mandar Joshi,\\nArmen Aghajanyan, Wen-tau Yih, Joelle Pineau, and\\nLuke Zettlemoyer. 2022. Improving passage re-\\ntrieval with zero-shot question generation.Victor Sanh, Albert Webson, Colin Raffel, Stephen\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey,\\nM Saiful Bari, Canwen Xu, Urmish Thakker,\\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\\nKim, Gunjan Chhablani, Nihal V . Nayak, De-\\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian\\nJiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\\nheesht Sharma, Andrea Santilli, Thibault Févry, Ja-\\nson Alan Fries, Ryan Teehan, Teven Le Scao, Stella\\nBiderman, Leo Gao, Thomas Wolf, and Alexan-\\nder M. Rush. 2022. Multitask prompted training\\nenables zero-shot task generalization. In The Tenth\\nInternational Conference on Learning Representa-\\ntions, ICLR 2022, Virtual Event, April 25-29, 2022 .\\nOpenReview.net.\\nYi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni,\\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\\nZhao, Jai Prakash Gupta, Tal Schuster, William W.\\nCohen, and Donald Metzler. 2022. Transformer\\nmemory as a differentiable search index. CoRR ,\\nabs/2202.06991.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\\nA heterogenous benchmark for zero-shot evalu-\\nation of information retrieval models. CoRR ,\\nabs/2104.08663.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\\nIgor Krivokon, Will Rusch, Marc Pickett, Kath-\\nleen S. Meier-Hellstern, Meredith Ringel Morris,\\nTulsee Doshi, Renelito Delos Santos, Toju Duke,\\nJohnny Soraker, Ben Zevenbergen, Vinodkumar\\nPrabhakaran, Mark Diaz, Ben Hutchinson, Kristen\\nOlson, Alejandra Molina, Erin Hoffman-John, Josh\\nLee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton,\\nAaron Cohen, Rachel Bernstein, Ray Kurzweil,\\nBlaise Aguera-Arcas, Claire Cui, Marian Croak,\\nEd H. Chi, and Quoc Le. 2022. Lamda: Lan-\\nguage models for dialog applications. CoRR ,\\nabs/2201.08239.\\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna\\nGurevych. 2022. GPL: Generative pseudo label-\\ning for unsupervised domain adaptation of dense re-\\ntrieval. In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies , pages 2345–2360, Seattle, United States.\\nAssociation for Computational Linguistics.\\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned lan-\\nguage models are zero-shot learners. In The Tenth\\nInternational Conference on Learning Representa-\\ntions, ICLR 2022, Virtual Event, April 25-29, 2022 .\\nOpenReview.net.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In 9th International Conference on Learning\\nRepresentations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021 . OpenReview.net.\\nYue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and\\nArnold Overwijk. 2022. Coco-dr: Combating dis-\\ntribution shifts in zero-shot dense retrieval with con-\\ntrastive and distributionally robust learning. In Pro-\\nceedings of the 2022 Conference on Empirical Meth-\\nods in Natural Language Processing .\\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.\\n2021. Mr. TyDi: A multi-lingual benchmark for\\ndense retrieval. arXiv:2108.08787 .\\nA Appendix\\nA.1 Instructions\\nA.1.1 Web Search\\nPlease write a passage to answer the question\\nQuestion: [QUESTION]\\nPassage:\\nA.1.2 SciFact\\nPlease write a scientiﬁc paper passage to support/refute the claim\\nClaim: [Claim]\\nPassage:\\nA.1.3 Arguana\\nPlease write a counter argument for the passage\\nPassage: [PASSAGE]\\nCounter Argument:\\nA.1.4 TREC-COVID\\nPlease write a scientiﬁc paper passage to answer the question\\nQuestion: [QUESTION]\\nPassage:\\nA.1.5 FiQA\\nPlease write a ﬁnancial article passage to answer the question\\nQuestion: [QUESTION]\\nPassage:\\nA.1.6 DBPedia-Entity\\nPlease write a passage to answer the question.\\nQuestion: [QUESTION]\\nPassage:\\nA.1.7 TREC-NEWS\\nPlease write a news passage about the topic.\\nTopic: [TOPIC]\\nPassage:\\nA.1.8 Mr.TyDi\\nPlease write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\\nQuestion: [QUESTION]\\nPassage:',\n",
       " 'arxiv_id': '2212-10496',\n",
       " 'chunks': ['Precise Zero-Shot Dense Retrieval without Relevance Labels Luyu Gao∗†Xueguang Ma∗‡Jimmy Lin‡Jamie Callan† †Language Technologies Institute, Carnegie Mellon University ‡David R. Cheriton School of Computer Science, University of Waterloo {luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca Abstract While dense retrieval has been shown effec- tive and efﬁcient across tasks and languages, it remains difﬁcult to create effective fully zero-shot dense retrieval systems when no rel- evance label is available. In this paper, we recognize the difﬁculty of zero-shot learning and encoding relevance. Instead, we pro- pose to pivot through Hy pothetical D ocument Embeddings ( HyDE ). Given a query, HyDE ﬁrst zero-shot instructs',\n",
       "  'and encoding relevance. Instead, we pro- pose to pivot through Hy pothetical D ocument Embeddings ( HyDE ). Given a query, HyDE ﬁrst zero-shot instructs an instruction-following language model (e.g. InstructGPT ) to gen- erate a hypothetical document. The docu- ment captures relevance patterns but is unreal and may contain false details. Then, an un- supervised contrastively learned encoder (e.g. Contriever ) encodes the document into an embedding vector. This vector identiﬁes a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder’s dense bottleneck ﬁltering out the incorrect details. Our exper- iments show that HyDE signiﬁcantly outper- forms the state-of-the-art unsupervised dense retriever Contriever',\n",
       "  'with the encoder’s dense bottleneck ﬁltering out the incorrect details. Our exper- iments show that HyDE signiﬁcantly outper- forms the state-of-the-art unsupervised dense retriever Contriever and shows strong per- formance comparable to ﬁne-tuned retrievers, across various tasks (e.g. web search, QA, fact veriﬁcation) and languages (e.g. sw, ko, ja).1 1 Introduction Dense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact veriﬁcation. A variety of meth- ods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021) and task-speciﬁc ∗Equal contribution. 1No models were trained or ﬁne-tuned in',\n",
       "  '2021), distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021) and task-speciﬁc ∗Equal contribution. 1No models were trained or ﬁne-tuned in making this pre- print. Our open source code is available at https://github. com/texttron/hyde .pre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models. On the other hand, zero-shot dense retrieval still remains difﬁcult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of',\n",
       "  'and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token',\n",
       "  'supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang',\n",
       "  'of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). Ouyang et al. (2022) show that with a small amount of data, GPT-3 (Brown et al., 2020) models can be alignedarXiv:2212.10496v1 [cs.IR] 20 Dec 2022 HyDE GPTContrieverhow long does it take to remove wisdom tooth It usually takes between 30 minutes and two hours to remove a wisdom tooth...How wisdom teeth are removed... Some ... a few minutes, whereas others can take 20 minutes or longer .... How has the COVID-19 pandemic impacted mental health?...depression and anxiety had increased by 20% since the start of',\n",
       "  'can take 20 minutes or longer .... How has the COVID-19 pandemic impacted mental health?...depression and anxiety had increased by 20% since the start of the pandemic...... two studies investigating COVID-19 patients ... significantly higher level of depressive ...write a passage to answer the question write a scientific paper passage to answer the question 인간은 언제 불을 사용했는가 ?write a passage in Korean to answer the question in detail인간이 불을 사용한 기록은 약 800 만년 전부터 나타난다 ...... 불을 처음 사용한 시기는 호모 에렉투스가 살았던 142 만 년 전으 로 거슬러간다 ... instruction query generated document real documentFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 andContriever /mContriever models. to human',\n",
       "  'illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 andContriever /mContriever models. to human intent to follow instructions. With these ingredients, we propose to pivot through Hypothetical Document Embeddings ( HyDE ), and decompose dense retrieval into two tasks, a generative task per- formed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In',\n",
       "  'process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder’s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are ﬁltered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks.',\n",
       "  'HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks. HyDE appears unsupervised. Nomodel is trained inHyDE : both the generative model and the con- trastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM. In our experiments, we show HyDE using Instruct- GPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models signiﬁcantly out- performs the previous state-of-the-art Contriever- only zero-shot no-relevance system on 11 queriessets, covering tasks like Web Search, Question Answering, Fact Veriﬁcation and languages like Swahili, Korean, Japanese. 2 Related Works Dense Retrieval (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after',\n",
       "  'Veriﬁcation and languages like Swahili, Korean, Japanese. 2 Related Works Dense Retrieval (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers stud- ied the metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sam- pling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021). Later works studied the second stage pre-training of language model speciﬁcally for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022). The popularity of dense retrieval can be partially attributed to the rich and',\n",
       "  'Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022). The popularity of dense retrieval can be partially attributed to the rich and successful research in very efﬁcient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2017). Instructions-Following Language Models Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with',\n",
       "  'standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with Instructions”. They ﬁne-tuned dense encoders that can also encode task-speciﬁc instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above. Zero-Shot Dense Retrieval The tasks of zero- shot (dense) retrieval are arguably empirically de- ﬁned by Thakur et al. (2021) for the neural re- trieval community. Their BEIR benchmark con- sists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and',\n",
       "  'many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,',\n",
       "  'prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly, as with Sachan et al. (2022), the only supervision resides in the LLM, in the pro- cessing of learning to follow instructions. Generative Retrieval Generative search is a new class of retrieval methods that use neural generative models as search indices (Metzler et al., 2021; Tay et al., 2022; Bevilacqua et al., 2022; Lee et al., 2022). These models use (constrained) decoding to generate document identiﬁers, such as id and sub-string, which map directly to realdocuments. They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search indices (Bevilacqua',\n",
       "  'realdocuments. They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search indices (Bevilacqua et al., 2022; Lee et al., 2022). In comparison, our method uses the standard MIPS index and requires no training or training data. Our generative model produces an intermediate hypothetical document to be fed into a dense encoder, instead of a real document. 3 Methodology In this section, we ﬁrst formally deﬁne the prob- lem of (zero-shot) dense retrieval. Then we will introduce how HyDE is designed to solve it.3.1 Preliminaries Dense retrieval models similarity between query and document with inner product similarity. Given a query qand document d, it uses two encoder function encqandencdto map them into ddimen- sion',\n",
       "  'between query and document with inner product similarity. Given a query qand document d, it uses two encoder function encqandencdto map them into ddimen- sion vectors vq,vd, whose inner product is used as similarity measurement. sim(q,d) =⟨encq(q),encd(d)⟩=⟨vq,vd⟩(1) For zero-shot retrieval, we consider Lquery sets Q1, Q2, ..., Q Land their corresponding search cor- pus, document sets D1, D2, ..., D L. Denote the j-th query from i-th set query set Qiasqij. We need to fully deﬁne mapping functions encqand encdwithout access to any query set Qi, document setDi, or any relevance judgment rij. The difﬁculty of zero-shot dense retrieval lies precisely in Equation 1: it requires learning of two embedding functions (for query and document re- spectively) into the same embedding space where inner product',\n",
       "  'precisely in Equation 1: it requires learning of two embedding functions (for query and document re- spectively) into the same embedding space where inner product captures relevance . Without rele- vance judgments/scores to ﬁt, learning becomes intractable. 3.2 HyDE HyDE circumvents the aforementioned learning problem by performing search in document- only embedding space that captures document- document similarity. This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021; Gao et al., 2021; Gao and Callan, 2022). We set document encoder encddirectly as a contrastive encoder enc con. f=encd=enc con (2) This function is also denoted as ffor simplic- ity. This unsupervised contrastive encoder will be shared by all incoming document corpus. vd=f(d)∀d∈D1∪D2∪...∪DL (3) To build the query vector, we consider in addition',\n",
       "  'simplic- ity. This unsupervised contrastive encoder will be shared by all incoming document corpus. vd=f(d)∀d∈D1∪D2∪...∪DL (3) To build the query vector, we consider in addition an instruction following LM, InstructLM. It takes a query qand a textual instruction INST and follows them to perform the task speciﬁed by INST . For simplicity, denote, g(q,INST) =InstructLM (q,INST) (4) Now we can use gto map queries to \"hypotheti- cal\" documents by sampling from g, setting INST to be“write a paragraph that answers the question” . The generated document is not real, can and is likely to be ungrounded factually (Brown et al., 2020; Thoppilan et al., 2022). We only re- quire it to capture relevance pattern. This is done by generating documents, i.e. providing exam- ples. Critically,',\n",
       "  '2020; Thoppilan et al., 2022). We only re- quire it to capture relevance pattern. This is done by generating documents, i.e. providing exam- ples. Critically, here we ofﬂoad relevance mod- eling from representation learning model to an NLG model that generalizes signiﬁcantly more eas- ily, naturally, and effectively (Brown et al., 2020; Ouyang et al., 2022). Generating examples also replaces explicit modeling of relevance scores. We can now encode the generated document using the document encoder f. Write, E[vqij] =E[f(g(qij,INST i))] (5) Formally, gdeﬁnes a probability distribution based on the chain rule. In this paper, we simply consider the expectation value, assuming the distribution of vqijis uni-modal, i.e. the query is not ambiguous. The study of ambiguous queries and diversity is left to future work.',\n",
       "  'value, assuming the distribution of vqijis uni-modal, i.e. the query is not ambiguous. The study of ambiguous queries and diversity is left to future work. We estimate Equation 5 by sampling Ndocuments from g,[ˆd1,ˆd2, ...,ˆdN]. ˆvqij=1 N∑ ˆdk∼g(qij,INST i)f(dk) (6) =1 NN∑ k=1f(ˆdk) (7) We also consider the query as a possible hypothesis, ˆvqij=1 N+ 1[N∑ k=1f(ˆdk) +f(qij)] (8) Inner product is computed between ˆvqijand the set of all document vectors {f(d)|d∈Di}. The most similar documents are retrieved. Here the encoder function fserves as a lossy compressor that outputs dense vectors, where the extra details are ﬁltered and left out from the vector. It further grounds the hypothetical vector to the actual corpus and the real documents. The full HyDE system is illustrated in Figure',\n",
       "  'from the vector. It further grounds the hypothetical vector to the actual corpus and the real documents. The full HyDE system is illustrated in Figure 1. 4 Experiments 4.1 Setup Implementation We implement HyDE using InstructGPT , a GPT-3 model from the instruct series (text-davinci-003 ; Ouyang et al. (2022)) andContriever models (Izacard et al., 2021). Wesample from InstructGPT using the OpenAI play- ground default temperature of 0.7 for open-ended generations. We use the English-only Contriever model for English retrieval tasks and multilingual mContriever for non-English tasks. We conducted retrieval experiments with the Pyserini toolkit (Lin et al., 2021a). Datasets We consider web search query sets TREC DL19 (Craswell et al., 2020a) and DL20 (Craswell et al., 2020b); they are based on the MS-MARCO dataset',\n",
       "  'We consider web search query sets TREC DL19 (Craswell et al., 2020a) and DL20 (Craswell et al., 2020b); they are based on the MS-MARCO dataset (Bajaj et al., 2016). We also use a diverse collection of 6 low-resource datasets from the BEIR dataset (Thakur et al., 2021). For non-English retrieval, we consider Swahili, Korean, Japanese, and Bengali from the Mr.Tydi dataset (Zhang et al., 2021). We use different instructions for each dataset. They share a similar structure but have different quantiﬁers to control the exact form of the gener- ated hypothetical documents. These instructions can be found in subsection A.1. Compared Systems Contriever models, Contriever andmContriever , serve as our major baseline. They are trained using unsupervised contrastive learning. HyDE retrievers share the exact same',\n",
       "  'Compared Systems Contriever models, Contriever andmContriever , serve as our major baseline. They are trained using unsupervised contrastive learning. HyDE retrievers share the exact same embedding spaces with them. The only difference is how the query vector is built. These comparisons allow us to easily examine the effect of HyDE . The classical heuristic-based lexical retriever BM25 is also included. Several systems that involve ﬁne-tuning on mas- sive relevance data are also included as refer- ences. We consider models ﬁne-tuned on MS- MARCO and transferred, DPR and ANCE, from the BEIR paper. For multilingual, we include the mDPR model from Mr.Tydi paper and MS- MARCO ﬁne-tuned mBERT and XLM-R from the Contriever paper. We also include the state-of- the-art transfer learning models: Contriever and mContriever',\n",
       "  'Mr.Tydi paper and MS- MARCO ﬁne-tuned mBERT and XLM-R from the Contriever paper. We also include the state-of- the-art transfer learning models: Contriever and mContriever ﬁne-tuned on MS-MARCO, denoted ContrieverFTandmContrieverFT. These mod- els have run through the state-of-the-art retrieval model training pipeline that involves second-stage retrieval-speciﬁc pre-training (Lee et al., 2019) and a few rounds of ﬁne-tuning (Qu et al., 2021); they should be considered empirical upper bounds. 4.2 Web Search In Table 1, we show retrieval results on TREC DL19 and TREC DL20. We see HyDE bring sizable improvements to Contriever across the board for DL19 DL20 map ndcg@10 recall@1k map ndcg@10 recall@1k w/o relevance judgement BM25 30.1 50.6 75.0 28.6 48.0 78.6 Contriever 24.0 44.5 74.6 24.0 42.1 75.4 HyDE 41.8 61.3 88.0',\n",
       "  'recall@1k map ndcg@10 recall@1k w/o relevance judgement BM25 30.1 50.6 75.0 28.6 48.0 78.6 Contriever 24.0 44.5 74.6 24.0 42.1 75.4 HyDE 41.8 61.3 88.0 38.2 57.9 84.4 w/ relevance judgement DPR 36.5 62.2 76.9 41.8 65.3 81.4 ANCE 37.1 64.5 75.5 40.8 64.6 77.6 ContrieverFT41.7 62.1 83.6 43.6 63.2 85.8 Table 1: Results for web search on DL19/20. Best performing w/o relevance and overall system(s) are marked bold . DPR, ANCE and ContrieverFTare in-domain supervised models that are ﬁnetuned on MS MARCO training data. Scifact Arguana Trec-Covid FiQA DBPedia TREC-NEWS nDCG@10 w/o relevance judgement BM25 67.9 39.7 59.5 23.6 31.8 39.5 Contriever 64.9 37.9 27.3 24.5 29.2 34.8 HyDE 69.1 46.6 59.3 27.3 36.8 44.0 w/ relevance judgement DPR 31.8 17.5 33.2 29.5 26.3',\n",
       "  '31.8 39.5 Contriever 64.9 37.9 27.3 24.5 29.2 34.8 HyDE 69.1 46.6 59.3 27.3 36.8 44.0 w/ relevance judgement DPR 31.8 17.5 33.2 29.5 26.3 16.1 ANCE 50.7 41.5 65.4 30.0 28.1 38.2 ContrieverFT67.7 44.6 59.6 32.9 41.3 42.8 Recall@100 w/o relevance judgement BM25 92.5 93.2 49.8 54.0 46.8 44.7 Contriever 92.6 90.1 17.2 56.2 45.3 42.3 HyDE 96.4 97.9 41.4 62.1 47.2 50.9 w/ relevance judgement DPR 72.7 75.1 21.2 34.2 34.9 21.5 ANCE 81.6 93.7 45.7 58.1 31.9 39.8 ContrieverFT94.7 97.7 40.7 65.6 54.1 49.2 Table 2: Low resource tasks from BEIR. Best performing w/o relevance and overall system(s) are marked bold . both precision-oriented and recall metrics. While unsupervised Contriever can underperform the classical BM25 approach, HyDE outperforms BM25 by large margins.',\n",
       "  'system(s) are marked bold . both precision-oriented and recall metrics. While unsupervised Contriever can underperform the classical BM25 approach, HyDE outperforms BM25 by large margins. HyDE remains competitive even when compared to ﬁne-tuned models. Note that TREC DL19/20 are search tasks deﬁned on MS-MARCO and there, all the ﬁne-tuned models are richly super- vised . On TREC DL19, HyDE shows comparable map and ndcg@10 to ContrieverFTand best re- call@1k. On DL20, HyDE gets around 10% lower map and ndcg@10 than ContrieverFTand sim- ilar recall@1k. The ANCE model shows better ndcg@10 numbers than HyDE but lower recall, sug- gesting it may be biased to a subset of queries and/or relevant documents.4.3 Low Resource Retrieval In Table 2, we show retrieval results on low- resource tasks from',\n",
       "  'be biased to a subset of queries and/or relevant documents.4.3 Low Resource Retrieval In Table 2, we show retrieval results on low- resource tasks from BEIR. Similar to web search,HyDE again brings sizable improvements to Contriever across the board in terms of both ndcg and recall. HyDE is only outperformed by BM25 on one dataset, TREC-Covid but with a tiny 0.2 mar- gin; in comparison, the underlying Contriever underperforms by more than 50%. We also observe HyDE demonstrates strong performance compared to ﬁne-tuned models. HyDE generally shows better performance than ANCE and DPR, even though the two are ﬁne-tuned on MS-MARCO and ANCE also in- volves some sophisticated hard negative techniques. ContrieverFTshows performance advantages on FiQA and DBPedia. These involve retrieval of ﬁ- nancial posts',\n",
       "  'MS-MARCO and ANCE also in- volves some sophisticated hard negative techniques. ContrieverFTshows performance advantages on FiQA and DBPedia. These involve retrieval of ﬁ- nancial posts or entities respectively. We believe the performance difference can be attributed to the Swahili Korean Japanese Bengali w/o relevance judgement BM25 38.9 28.5 21.2 41.8 mContriever 38.3 22.3 19.5 35.3 HyDE 41.7 30.6 30.7 41.3 w/ relevance judgement mDPR 7.3 21.9 18.1 25.8 mBERT 37.4 28.1 27.1 35.1 XLM-R 35.1 32.2 24.8 41.7 mContrieverFT51.2 34.2 32.4 42.3 Table 3: MRR@100 on Mr.Tydi. Best performing w/o relevance and overall system(s) are marked bold . under-speciﬁcation of the instruction; more elabo- rative instructions may help. 4.4 Multilingual Retrieval Multilingual setup poses several additional chal- lenges to HyDE . The small-sized contrastive en-',\n",
       "  'the instruction; more elabo- rative instructions may help. 4.4 Multilingual Retrieval Multilingual setup poses several additional chal- lenges to HyDE . The small-sized contrastive en- coder gets saturated as the number of languages scales (Conneau et al., 2020; Izacard et al., 2021). Meanwhile, our generative LLM faces an opposite issue: with languages of not as high resource as English or French, the high capacity LLM can get under-trained (Hoffmann et al., 2022). Nevertheless, in Table 3, we still ﬁnd HyDE able to improve the mContriever model. It can outperform non-Contriever models ﬁne-tuned on and transferred from MS-MARCO. On the other hand, we do observe some margins between HyDE and ﬁne-tuned mContrieverFT. SinceHyDE and mContrieverFTuse similar contrastive encoders, we hypothesize this is because the non-English lan-',\n",
       "  'hand, we do observe some margins between HyDE and ﬁne-tuned mContrieverFT. SinceHyDE and mContrieverFTuse similar contrastive encoders, we hypothesize this is because the non-English lan- guages we considered are under-trained in both pre-training and instruction learning stages. 5 Analysis The generative LLM and contrastive encoder make up the backbone of HyDE . In this section, we study the effect of changing their realizations. In partic- ular, we consider smaller language models (LM) and ﬁne-tuned encoders. We conduct our studies on TREC DL19/20. 5.1 Effect of Different Generative Models In Table 4, we show HyDE using other instruction-following language models. In particular, we consider a 52-billion Cohere model ( command-xlarge-20221108 ) and a 11-billion FLAN model ( FLAN-T5-xxl ; Wei et al. (2022)).2Generally, we observe that',\n",
       "  'particular, we consider a 52-billion Cohere model ( command-xlarge-20221108 ) and a 11-billion FLAN model ( FLAN-T5-xxl ; Wei et al. (2022)).2Generally, we observe that all 2Model sizes are from https://crfm.stanford.edu/ helm/v1.0/?models .Model DL19 DL20 Contriever 44.5 42.1 ContrieverFT62.1 63.2 HyDE w/ Contriever w/ Flan-T5 (11b) 48.9 52.9 w/ Cohere (52b) 53.8 53.8 w/ GPT (175b) 61.3 57.9 w/ ContrieverFT w/ Flan-T5 (11b) 60.2 62.1 w/ Cohere (52b) 61.4 63.1 w/ GPT (175b) 67.4 63.5 Table 4: NDCG@10 on TREC DL19/20. Effect of changing different instruction LMs and using ﬁne- tuned encoder. Best w/o relevance and overall models are marked bold . models bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the',\n",
       "  'bold . models bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the Cohere model is still experimental without much detail disclosed. We can only tentatively hypothesize that training techniques may have also played some role in the performance difference. 5.2 HyDE with Fine-tuned Encoder To begin with, HyDE with ﬁne-tuned encoder is notthe intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present. Here we are interested to ﬁnd out if and howHyDE embedding can affect ﬁne-tuned en- coders. In Table 4, we see that less powerful instruc- tion LMs can negatively impact the overall perfor- mance of the ﬁne-tuned retriever. (To remind our readers,ContrieverFTis in-domain supervisedly ﬁne-tuned',\n",
       "  'see that less powerful instruc- tion LMs can negatively impact the overall perfor- mance of the ﬁne-tuned retriever. (To remind our readers,ContrieverFTis in-domain supervisedly ﬁne-tuned for TREC DL19/20). The performance degradations remain small. On the other hand, we also observe the InstructGPT model able to fur- ther bring up the performance, especially on DL19. This suggests that there may still exist certain fac- tors not captured by the ﬁne-tuned encoder but only by the generative model. 6 Conclusion At the end of the paper, we encourage the readers to take a moment and reﬂect on the HyDE model. Compare it to some of the other recently seen re- trievers or re-ranker. These other models probably differ in their architecture, training method, and/or task, but probably',\n",
       "  'to some of the other recently seen re- trievers or re-ranker. These other models probably differ in their architecture, training method, and/or task, but probably all of them involve modeling relevance scores between a pair of query and docu- ment. Dense retrievers consider vector similarities while self-attentive re-rankers regression scores. In comparison, the concept of relevance in HyDE is captured by an NLG model and the language gener- ation process. We demonstrate in many cases, HyDE can be as effective as dense retrievers that learn to model numerical relevance scores. So, is numeri- cal relevance just a statistical artifact of language understanding? Will a weak retriever theoretically sufﬁce as the NLU & NLG models rapidly become stronger? Rushing to conclusions is not smart; more works',\n",
       "  'language understanding? Will a weak retriever theoretically sufﬁce as the NLU & NLG models rapidly become stronger? Rushing to conclusions is not smart; more works need to be done to get answers. With this paper, we just want to raise these questions. Concretely in this paper, we introduce a new paradigm of interactions between LLM and dense encoder/retriever. We demonstrate (part of) rel- evance modeling and instruction understanding can be delegated to the more powerful and ﬂex- ible LLM. As a consequence, the need for rele- vance labels is removed. We are excited to see how this can be generalized further to more so- phisticated tasks like multi-hop retrieval/QA and conversational search. We argue HyDE is also of practical use though not necessarily over the',\n",
       "  'further to more so- phisticated tasks like multi-hop retrieval/QA and conversational search. We argue HyDE is also of practical use though not necessarily over the entire lifespan of a search sys- tem. At the very beginning of the life of the search system, serving queries using HyDE offers perfor- mance comparable to a ﬁne-tuned model, which no other relevance-free model can offer. As the search log grows, a supervised dense retriever can be gradually rolled out. As the dense retriever grows stronger, more queries will be routed to it, with only less common and emerging ones going toHyDE backend. References Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Ha- jishirzi, and Wen-tau Yih. 2022. Task-aware re- trieval with instructions. Payal',\n",
       "  'Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Ha- jishirzi, and Wen-tau Yih. 2022. Task-aware re- trieval with instructions. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti- wary, and Tong Wang. 2016. Ms marco: A human generated machine reading comprehension dataset. Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive search engines: Gen- erating substrings as document identiﬁers. CoRR , abs/2204.10628.Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,',\n",
       "  'Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc- Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learn- ers. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Informa- tion Processing Systems 2020, NeurIPS 2020, De- cember 6-12, 2020, virtual . Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka- plan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish',\n",
       "  'de Oliveira Pinto, Jared Ka- plan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welin- der, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Eval- uating large language models',\n",
       "  'Brundage, Mira Murati, Katie Mayer, Peter Welin- der, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Eval- uating large language models trained on code. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin- odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe- mawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fe- dus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark',\n",
       "  'Misra, Kevin Robinson, Liam Fe- dus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankara- narayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren- nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettle- moyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451,',\n",
       "  '2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)',\n",
       "  'Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Luyu Gao and Jamie Callan. 2021. Condenser: a pre- training architecture for dense retrieval. In Proceed- ings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 981–993, Online and Punta Cana, Dominican Republic. Asso- ciation for Computational Linguistics. Luyu Gao and Jamie Callan. 2022. Unsupervised cor- pus aware language model pre-training for dense passage retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , pages 2843–2853, Dublin, Ireland. Association for Computational Lin- guistics. Tianyu Gao, Xingcheng',\n",
       "  'Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , pages 2843–2853, Dublin, Ireland. Association for Computational Lin- guistics. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing, pages 6894–6910, Online and Punta Cana, Do- minican Republic. Association for Computational Linguistics. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Si- monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. Sebastian Hofstätter, Sheng-Chieh',\n",
       "  'Guy, Simon Osindero, Karen Si- monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Ef- ﬁciently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval ,SIGIR ’21, page 113–122, New York, NY , USA. As- sociation for Computing Machinery. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se- bastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. CoRR , abs/2112.09118. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. CoRR , abs/1702.08734. Vladimir Karpukhin, Barlas Oguz, Sewon',\n",
       "  'learning. CoRR , abs/2112.09118. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. CoRR , abs/1702.08734. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP) , pages 6769– 6781, Online. Association for Computational Lin- guistics. Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative multi-hop retrieval. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics , pages 6086–6096, Florence, Italy. Association for Computational Linguistics. Jimmy Lin,',\n",
       "  'In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics , pages 6086–6096, Florence, Italy. Association for Computational Linguistics. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng- Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021) , pages 2356–2362. Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021b. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021) , pages 163– 173, Online. Association for Computational Linguis- tics. Zheng Liu and Yingxia Shao. 2022. Retromae: Pre- training retrieval-oriented',\n",
       "  'Representation Learning for NLP (RepL4NLP-2021) , pages 163– 173, Online. Association for Computational Linguis- tics. Zheng Liu and Yingxia Shao. 2022. Retromae: Pre- training retrieval-oriented transformers via masked auto-encoder. ArXiv , abs/2205.12035. Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. 2021. Less is more: Pre- train a strong Siamese encoder for dense text re- trieval using a weak decoder. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 2780–2791, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search: making domain experts out of dilettantes. SIGIR Forum , 55(1):13:1–13:27. Sewon Min, Mike Lewis, Luke Zettlemoyer, and',\n",
       "  'Dara Bahri, and Marc Najork. 2021. Rethinking search: making domain experts out of dilettantes. SIGIR Forum , 55(1):13:1–13:27. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han- naneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies , pages 2791–2809, Seattle, United States. Association for Computational Linguistics. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe- ter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow in- structions with human feedback. Yingqi Qu, Yuchen Ding, Jing',\n",
       "  'ter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow in- structions with human feedback. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An opti- mized training approach to dense passage retrieval for open-domain question answering. In Proceed- ings of the 2021 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5835–5847, Online. Association for Computational Linguistics. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, Eliza Rutherford, Tom Hennigan, Ja- cob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,',\n",
       "  'Sarah Henderson, Roman Ring, Susan- nah Young, Eliza Rutherford, Tom Hennigan, Ja- cob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Mari- beth Rauh, Po-Sen Huang, Amelia Glaese, Jo- hannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, An- tonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Ne- matzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cy- prien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy,',\n",
       "  'Zhitao Gong, Daniel Toyama, Cy- prien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hecht- man, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan- way, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scal- ing language models: Methods, analysis & insights from training gopher. Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage re- trieval with zero-shot question generation.Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu,',\n",
       "  'generation.Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, De- bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Ab- heesht Sharma, Andrea Santilli, Thibault Févry, Ja- son Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexan- der M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. Yi Tay, Vinh Q. Tran,',\n",
       "  'task generalization. In The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer memory as a differentiable search index. CoRR , abs/2202.06991. Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab- hishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogenous benchmark for zero-shot evalu- ation of information retrieval models. CoRR , abs/2104.08663. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun,',\n",
       "  'Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kath- leen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. Lamda: Lan- guage models for dialog applications. CoRR , abs/2201.08239. Kexin Wang, Nandan',\n",
       "  'Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. Lamda: Lan- guage models for dialog applications. CoRR , abs/2201.08239. Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative pseudo label- ing for unsupervised domain adaptation of dense re- trieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies , pages 2345–2360, Seattle, United States. Association for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M. Dai, and Quoc V . Le. 2022. Finetuned lan- guage models are zero-shot learners. In The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29,',\n",
       "  '. Le. 2022. Finetuned lan- guage models are zero-shot learners. In The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neigh- bor negative contrastive learning for dense text re- trieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net. Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and Arnold Overwijk. 2022. Coco-dr: Combating dis- tribution shifts in zero-shot dense retrieval with con- trastive and distributionally robust learning. In Pro- ceedings of the 2022 Conference on Empirical Meth- ods in Natural Language Processing . Xinyu Zhang, Xueguang Ma, Peng',\n",
       "  'and distributionally robust learning. In Pro- ceedings of the 2022 Conference on Empirical Meth- ods in Natural Language Processing . Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. TyDi: A multi-lingual benchmark for dense retrieval. arXiv:2108.08787 . A Appendix A.1 Instructions A.1.1 Web Search Please write a passage to answer the question Question: [QUESTION] Passage: A.1.2 SciFact Please write a scientiﬁc paper passage to support/refute the claim Claim: [Claim] Passage: A.1.3 Arguana Please write a counter argument for the passage Passage: [PASSAGE] Counter Argument: A.1.4 TREC-COVID Please write a scientiﬁc paper passage to answer the question Question: [QUESTION] Passage: A.1.5 FiQA Please',\n",
       "  'for the passage Passage: [PASSAGE] Counter Argument: A.1.4 TREC-COVID Please write a scientiﬁc paper passage to answer the question Question: [QUESTION] Passage: A.1.5 FiQA Please write a ﬁnancial article passage to answer the question Question: [QUESTION] Passage: A.1.6 DBPedia-Entity Please write a passage to answer the question. Question: [QUESTION] Passage: A.1.7 TREC-NEWS Please write a news passage about the topic. Topic: [TOPIC] Passage: A.1.8 Mr.TyDi Please write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail. Question: [QUESTION] Passage:']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_2212 = get_chunks_from_paper(\"https://arxiv.org/pdf/2212.10496.pdf\")\n",
    "chunked_2212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a tenant for the paper - `create_tenant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.tenants import Tenant\n",
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "def create_tenant(chunked_paper):\n",
    "    tenant_name = chunked_paper[\"arxiv_id\"]\n",
    "\n",
    "    papers.tenants.create([\n",
    "        Tenant(name=tenant_name)\n",
    "    ])\n",
    "\n",
    "    return tenant_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-19 00:22:52,080] INFO in _client: HTTP Request: POST http://127.0.0.1:8080/v1/schema/Papers/tenants \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2212-10496'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_tenant(chunked_2212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2212-10496': TenantOutput(name='2212-10496', activityStatusInternal=<TenantActivityStatus.ACTIVE: 'ACTIVE'>, activityStatus=<_TenantActivistatusServerValues.HOT: 'HOT'>)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.tenants.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch import chunks - `batch_import_chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_import_chunks(chunked_paper):\n",
    "    ten = papers.with_tenant(chunked_paper[\"arxiv_id\"])\n",
    "\n",
    "    i=0\n",
    "    with ten.batch.dynamic() as batch:\n",
    "        for chunk in chunked_paper[\"chunks\"]:\n",
    "            batch.add_object({\n",
    "                \"title\": chunked_paper[\"title\"],\n",
    "                \"url\": chunked_paper[\"url\"],\n",
    "                \"chunk\": chunk,\n",
    "                \"chunk_no\": i,\n",
    "            })\n",
    "            i+=1\n",
    "\n",
    "    # if(len(papers.batch.failed_objects)>0):\n",
    "    if(len(ten.batch.failed_objects)>0):\n",
    "        print(\"Import complete with errors\")\n",
    "        for err in papers.batch.failed_objects:\n",
    "            print(err)\n",
    "    else:\n",
    "        print(\"Import complete with no errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-19 00:22:52,141] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/schema/Papers \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:52,147] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:53,159] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:54,186] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:55,191] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:56,197] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:57,199] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:58,205] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:22:59,210] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:00,215] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:01,221] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:02,228] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:03,237] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:04,244] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:05,252] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import complete with no errors\n"
     ]
    }
   ],
   "source": [
    "batch_import_chunks(chunked_2212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end paper load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_paper(url):\n",
    "    cp = get_chunks_from_paper(url)\n",
    "    tenant_name = create_tenant(cp)\n",
    "    batch_import_chunks(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-19 00:23:06,239] INFO in text: Getting arXiV paper from https://arxiv.org/pdf/2401.00107.pdf\n",
      "[2025-09-19 00:23:06,240] INFO in utils: Getting arXiV title from https://arxiv.org/abs/2401.00107\n",
      "[2025-09-19 00:23:06,315] INFO in utils: Chunking text of 47862 characters with words method.\n",
      "[2025-09-19 00:23:06,317] INFO in utils: Chunking text of 47618 chars by number of words.\n",
      "[2025-09-19 00:23:06,323] INFO in _client: HTTP Request: POST http://127.0.0.1:8080/v1/schema/Papers/tenants \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:06,325] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/schema/Papers \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:06,328] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:07,331] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:08,339] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:09,344] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:10,346] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:11,354] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:12,357] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:13,363] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:14,365] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:15,370] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:16,375] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:17,382] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:18,391] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:19,399] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:20,404] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:21,412] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "[2025-09-19 00:23:22,415] INFO in _client: HTTP Request: GET http://127.0.0.1:8080/v1/nodes \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import complete with no errors\n"
     ]
    }
   ],
   "source": [
    "import_paper(\"https://arxiv.org/pdf/2401.00107.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
