{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-tenant Chat with Papers - Query papers\n",
    "## Get keys and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwzujlbgtk\n",
      "L1FUbzRPaU\n",
      "sk-proj-iu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n",
    "WEAVIATE_KEY = os.getenv(\"WEAVIATE_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(WEAVIATE_URL[:10])\n",
    "print(WEAVIATE_KEY[:10])\n",
    "print(OPENAI_API_KEY[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "# from weaviate.classes.init import AdditionalConfig, Timeout\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=WEAVIATE_URL,\n",
    "    auth_credentials=Auth.api_key(WEAVIATE_KEY),\n",
    "\n",
    "    headers = {\n",
    "        \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
    "    },\n",
    "\n",
    "    # additional_config=AdditionalConfig(\n",
    "    #     timeout=Timeout(init=2, query=45, insert=120),  # Values in seconds\n",
    "    # )\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector search on tenants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,\n",
      "supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang\n",
      "and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token\n",
      "precisely in Equation 1: it requires learning of two embedding functions (for query and document re- spectively) into the same embedding space where inner product captures relevance . Without rele- vance judgments/scores to ﬁt, learning becomes intractable. 3.2 HyDE HyDE circumvents the aforementioned learning problem by performing search in document- only embedding space that captures document- document similarity. This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021; Gao et al., 2021; Gao and Callan, 2022). We set document encoder encddirectly as a contrastive encoder enc con. f=encd=enc con (2) This function is also denoted as ffor simplic- ity. This unsupervised contrastive encoder will be shared by all incoming document corpus. vd=f(d)∀d∈D1∪D2∪...∪DL (3) To build the query vector, we consider in addition\n",
      "2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten.query.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Search with tenants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,\n",
      "The text describes a research study focusing on the development of effective dense retrieval systems in the context of Transfer Learning. It emphasizes the challenge of building such systems without relying on relevance labels or a large, supervised corpus like MS-MARCO. The authors aim to explore methods for creating retrieval systems under more realistic conditions, where access to labeled data is limited or non-existent. This approach aligns with unsupervised learning paradigms, as noted by their reference to previous work, and seeks to avoid overfitting to specific test datasets. The overall goal is to improve the implementation of retrieval systems in scenarios where labeled data is not available. \n",
      "\n",
      "supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang\n",
      "The text describes concepts related to self-supervised representation learning methods within the context of modern deep learning, particularly focusing on natural language processing (NLP). It emphasizes two main approaches: \n",
      "\n",
      "1. **Token-Level Learning**: This involves generative large language models (LLMs) that are pre-trained on extensive text corpora. These models exhibit strong capabilities in natural language understanding (NLU) and natural language generation (NLG).\n",
      "\n",
      "2. **Document-Level Learning**: This section discusses text encoders that use contrastive learning methods to understand and encode the similarities between different documents (or text chunks). \n",
      "\n",
      "The text also notes that LLMs that have been further fine-tuned to follow instructions can perform well in a zero-shot manner—meaning they can handle new, unseen tasks or instructions without additional training specifically for those tasks. \n",
      "\n",
      "Overall, the passage indicates an exploration of advancing NLP technologies through self-supervised learning approaches and highlights the efficacy and versatility of LLMs in understanding and generating language. \n",
      "\n",
      "and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token\n",
      "The text describes research focusing on zero-shot dense retrieval systems in information retrieval. It discusses the limitations of existing datasets like MS-MARCO, which, while widely used, have restrictions on commercial use and may not be applicable to all real-world scenarios. The authors aim to create retrieval systems that do not rely on relevance supervision, meaning they won't require labeled data to function effectively. Instead, they plan to employ self-supervised representation learning methods, leveraging modern deep learning techniques to enhance the performance and generalization of these retrieval systems across different tasks without the need for prior relevant data. \n",
      "\n",
      "precisely in Equation 1: it requires learning of two embedding functions (for query and document re- spectively) into the same embedding space where inner product captures relevance . Without rele- vance judgments/scores to ﬁt, learning becomes intractable. 3.2 HyDE HyDE circumvents the aforementioned learning problem by performing search in document- only embedding space that captures document- document similarity. This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021; Gao et al., 2021; Gao and Callan, 2022). We set document encoder encddirectly as a contrastive encoder enc con. f=encd=enc con (2) This function is also denoted as ffor simplic- ity. This unsupervised contrastive encoder will be shared by all incoming document corpus. vd=f(d)∀d∈D1∪D2∪...∪DL (3) To build the query vector, we consider in addition\n",
      "The text describes a method for embedding learning in the context of information retrieval, specifically referring to an approach named \"HyDE.\" The primary focus is on learning two embedding functions for queries and documents in such a way that they are represented in the same embedding space, allowing the inner product to capture their relevance.\n",
      "\n",
      "The text highlights a limitation in traditional methods where learning these embeddings typically relies on having relevance judgments or scores, which can be difficult to obtain, making the learning process intractable without them.\n",
      "\n",
      "HyDE addresses this challenge by employing an unsupervised contrastive learning approach to create a document-only embedding space that focuses on document-document similarity. This strategy allows the model to be trained without needing explicit relevance scores by comparing documents to one another in an unsupervised manner.\n",
      "\n",
      "The text also mentions the use of a document encoder, specifically a contrastive encoder, which will be consistently applied across all documents in the corpus being analyzed. Additionally, it implies that query vectors will be built through a specific process which is not fully elaborated in the provided snippet but likely continues to relate to the unsupervised nature of the learning process introduced by HyDE. \n",
      "\n",
      "2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\n",
      "The text describes a collection of academic references primarily focused on advancements in natural language processing (NLP) and representation learning. Specifically, it mentions works that explore:\n",
      "\n",
      "1. **Unsupervised Cross-Lingual Representation Learning**: The first reference discusses a paper presented at the 58th Annual Meeting of the Association for Computational Linguistics in 2020, which covers methods for learning language representations across different languages without supervised data.\n",
      "\n",
      "2. **TREC Deep Learning Tracks**: The next two references (2020a and 2020b) provide overviews of the TREC (Text Retrieval Conference) deep learning tracks for 2019 and 2020, which likely detail evaluations and progress in deep learning methods applied to information retrieval.\n",
      "\n",
      "3. **BERT**: The final reference discusses BERT (Bidirectional Encoder Representations from Transformers), a significant model introduced in 2019 for understanding human language through deep learning. This paper has greatly influenced various NLP tasks by introducing pre-training techniques and transformer-based architecture.\n",
      "\n",
      "Overall, the text highlights key milestones in the field of NLP, particularly focusing on representation learning techniques and contributions made at major conferences. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten2212 = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten2212.generate.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=5,\n",
    "    single_prompt=\"What does the following text describe: {chunk}\",\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])\n",
    "    print(item.generative.text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,\n",
      "supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang\n",
      "and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token\n",
      "precisely in Equation 1: it requires learning of two embedding functions (for query and document re- spectively) into the same embedding space where inner product captures relevance . Without rele- vance judgments/scores to ﬁt, learning becomes intractable. 3.2 HyDE HyDE circumvents the aforementioned learning problem by performing search in document- only embedding space that captures document- document similarity. This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021; Gao et al., 2021; Gao and Callan, 2022). We set document encoder encddirectly as a contrastive encoder enc con. f=encd=enc con (2) This function is also denoted as ffor simplic- ity. This unsupervised contrastive encoder will be shared by all incoming document corpus. vd=f(d)∀d∈D1∪D2∪...∪DL (3) To build the query vector, we consider in addition\n",
      "2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\n",
      "Unsupervised learning operates in scenarios where there is no labeled data available for training. In the context discussed, researchers aimed to develop effective dense retrieval systems without relying on any relevance labels, indicating a shift away from traditional supervised methods.\n",
      "\n",
      "In this setup, unsupervised learning is often characterized by self-supervised representation learning. This approach utilizes modern deep learning techniques, which can be carried out at two different levels: token and document. At the token level, generative large language models (LLMs) pre-trained on extensive corpuses have shown robust capabilities in natural language understanding and generation. These models can interpret and generate human language effectively without specific instruction.\n",
      "\n",
      "At the document level, text encoders are trained using contrastive objectives. This method allows the models to learn how to encode the similarity between different documents, even when explicit relevance judgments are not available. Thus, the learning focuses on understanding the relationships between documents rather than mapping queries to specific document relevance scores.\n",
      "\n",
      "An example of an innovation in this area is the HyDE model, which navigates the challenges of learning representations without direct relevance feedback by focusing on the similarity between documents. It uses an unsupervised contrastive learning approach to encode documents, making it adaptable to various document collections without requiring labeled data.\n",
      "\n",
      "In summary, unsupervised learning utilizes self-supervised methods to derive meaning and relationships from data without supervision, applying techniques like contrastive learning to learn embeddings for documents and queries effectively. This enables systems to function in practical applications where labeled datasets are scarce.\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten2212 = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten2212.generate.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=5,\n",
    "    grouped_task=\"Explain how unsupervised learning works. Use only the provided content.\",\n",
    "    grouped_properties=[\"chunk\"]\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])\n",
    "\n",
    "print(response.generative.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_rag(paper_id, query, prompt):\n",
    "    papers = client.collections.get(\"Papers\")\n",
    "    ten = papers.with_tenant(paper_id)\n",
    "\n",
    "    response = ten.generate.near_text(\n",
    "        query=query,\n",
    "        limit=5,\n",
    "        grouped_task=prompt + \" Use only the provided content.\",\n",
    "        grouped_properties=[\"chunk\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"title\": response.objects[0].properties[\"title\"],\n",
    "        \"source\": [p.properties[\"chunk\"] for p in response.objects],\n",
    "        \"generated\": response.generative.text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Precise Zero-Shot Dense Retrieval without Relevance Labels',\n",
       " 'source': ['many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised” . Strictly,',\n",
       "  'supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang',\n",
       "  'and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token',\n",
       "  'precisely in Equation 1: it requires learning of two embedding functions (for query and document re- spectively) into the same embedding space where inner product captures relevance . Without rele- vance judgments/scores to ﬁt, learning becomes intractable. 3.2 HyDE HyDE circumvents the aforementioned learning problem by performing search in document- only embedding space that captures document- document similarity. This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021; Gao et al., 2021; Gao and Callan, 2022). We set document encoder encddirectly as a contrastive encoder enc con. f=encd=enc con (2) This function is also denoted as ffor simplic- ity. This unsupervised contrastive encoder will be shared by all incoming document corpus. vd=f(d)∀d∈D1∪D2∪...∪DL (3) To build the query vector, we consider in addition',\n",
       "  '2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)'],\n",
       " 'generated': 'Unsupervised learning operates in scenarios where labeled data is not available for training. This approach is particularly useful in practical applications where obtaining large, labeled datasets is challenging. Instead of relying on explicit labels or supervision, unsupervised learning utilizes the inherent structure and patterns within the data.\\n\\nIn the context discussed, the researchers focus on building dense retrieval systems without relevance labels, acknowledging the limitations of traditional methodologies that depend on extensive supervised collections, such as MS-MARCO. They propose an approach that begins with self-supervised representation learning methods, which leverage the power of modern deep learning.\\n\\nThe learning process involves the development of embedding functions for both queries and documents, placing them in a shared embedding space where their similarity can be assessed via inner products. However, without relevance judgments to guide the learning, traditional methods face challenges.\\n\\nTo address these challenges, the researchers introduce a method called HyDE, which relies on unsupervised contrastive learning. This technique allows the system to learn document-document similarities without requiring labeled data. By encoding documents in a way that captures their relative closeness or similarity, the system can effectively create representations that enable retrieval tasks. \\n\\nUltimately, unsupervised learning through methods like contrastive learning allows for the creation of effective models that generalize across various tasks without the need for explicit supervision or labeled datasets, making it a practical solution for real-world applications.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_rag(\n",
    "    \"2212-10496\",\n",
    "    \"Unsupervised learning\",\n",
    "    \"Explain how unsupervised learning works\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2212-10496': TenantOutput(name='2212-10496', activityStatusInternal=<TenantActivityStatus.ACTIVE: 'ACTIVE'>, activityStatus=<_TenantActivistatusServerValues.HOT: 'HOT'>),\n",
       " '2401-00107': TenantOutput(name='2401-00107', activityStatusInternal=<TenantActivityStatus.ACTIVE: 'ACTIVE'>, activityStatus=<_TenantActivistatusServerValues.HOT: 'HOT'>)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "papers.tenants.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
