{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-tenant Chat with Papers - Query papers\n",
    "## Get keys and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate Key:root-user-key\n",
      "OpenAI API Key: sk-dummy-key-for-local-testing\n",
      "OpenAI URL: http://host.docker.internal:11434\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WEAVIATE_KEY = os.getenv(\"WEAVIATE_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_URL = os.getenv(\"OPENAI_URL\")\n",
    "\n",
    "print(f\"Weaviate Key:{WEAVIATE_KEY}\")\n",
    "print(f\"OpenAI API Key: {OPENAI_API_KEY}\")\n",
    "print(f\"OpenAI URL: {OPENAI_URL}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Connect to the local instance\n",
    "client = weaviate.connect_to_local(\n",
    "  host=\"127.0.0.1\", # the address to the learner's instance\n",
    "  port=8080,\n",
    "  grpc_port=50051,\n",
    "  auth_credentials=Auth.api_key(WEAVIATE_KEY),\n",
    "  headers={\n",
    "    \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
    "  }\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector search on tenants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\n",
      "standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with Instructions”. They ﬁne-tuned dense encoders that can also encode task-speciﬁc instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above. Zero-Shot Dense Retrieval The tasks of zero- shot (dense) retrieval are arguably empirically de- ﬁned by Thakur et al. (2021) for the neural re- trieval community. Their BEIR benchmark con- sists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and\n",
      "bold . models bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the Cohere model is still experimental without much detail disclosed. We can only tentatively hypothesize that training techniques may have also played some role in the performance difference. 5.2 HyDE with Fine-tuned Encoder To begin with, HyDE with ﬁne-tuned encoder is notthe intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present. Here we are interested to ﬁnd out if and howHyDE embedding can affect ﬁne-tuned en- coders. In Table 4, we see that less powerful instruc- tion LMs can negatively impact the overall perfor- mance of the ﬁne-tuned retriever. (To remind our readers,ContrieverFTis in-domain supervisedly ﬁne-tuned\n",
      "generation.Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, De- bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Ab- heesht Sharma, Andrea Santilli, Thibault Févry, Ja- son Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexan- der M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. Yi Tay, Vinh Q. Tran,\n",
      "supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten.query.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Search with tenants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\n",
      "This text describes a research paper titled \"BERT\" that was presented at the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. The paper proposes an approach to supervised cross-lingual representation learning using deep bidirectional transformers, known as BERT (Bidirectional Encoder Representations from Transformers). \n",
      "\n",
      "standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with Instructions”. They ﬁne-tuned dense encoders that can also encode task-speciﬁc instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above. Zero-Shot Dense Retrieval The tasks of zero- shot (dense) retrieval are arguably empirically de- ﬁned by Thakur et al. (2021) for the neural re- trieval community. Their BEIR benchmark con- sists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and\n",
      "This text appears to be discussing two research papers related to sequence-to-sequence learning, specifically in the context of retrieval tasks:\n",
      "\n",
      "1. Standard supervised sequence-to-sequence learning.\n",
      "2. Zero-Shot Dense Retrieval (which refers to dense retrieval with no prior knowledge or task information).\n",
      "\n",
      "The text also mentions \"unsupervised encoding\" and a \"instruction following generative LLM\", which suggests that one paper uses an unsupervised approach, while the other likely uses a supervised approach.\n",
      "\n",
      "Additionally, the text talks about two specific papers:\n",
      "\n",
      "1. Ouyang et al. (2022) - titled \"Standard supervised sequence-to-sequence learning\"\n",
      "2. Asai et al. (2022) - titled \"Task-aware Retrieval with Instructions\"\n",
      "\n",
      "These papers seem to be discussing how to improve or extend these basic concepts in the field of retrieval tasks. \n",
      "\n",
      "bold . models bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the Cohere model is still experimental without much detail disclosed. We can only tentatively hypothesize that training techniques may have also played some role in the performance difference. 5.2 HyDE with Fine-tuned Encoder To begin with, HyDE with ﬁne-tuned encoder is notthe intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present. Here we are interested to ﬁnd out if and howHyDE embedding can affect ﬁne-tuned en- coders. In Table 4, we see that less powerful instruc- tion LMs can negatively impact the overall perfor- mance of the ﬁne-tuned retriever. (To remind our readers,ContrieverFTis in-domain supervisedly ﬁne-tuned\n",
      "This text appears to describe a research paper or study on improvement techniques for a machine learning model called \"Contriever\" or \"Cohere\". The specific focus is on whether larger models with more complex training data (e.g. using additional labels) can bring about greater improvements in the performance of a particular fine-tuned model, specifically one that uses the HyDE technique. \n",
      "\n",
      "generation.Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, De- bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Ab- heesht Sharma, Andrea Santilli, Thibault Févry, Ja- son Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexan- der M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. Yi Tay, Vinh Q. Tran,\n",
      "The text appears to describe a research paper or presentation titled \"Multitask Prompted Training Enables Zero-Shot Task Generalization\" by several researchers, including Victor Sanh, Albert Webson, Colin Raffel, and others. The paper was presented at the 10th International Conference on Learning Representations (ICLR) in April 2022, although it's noted to be a virtual event due to COVID-19 pandemic restrictions. \n",
      "\n",
      "supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang\n",
      "The text describes how deep learning models, particularly large language models (LLM), have been pre-trained on large corpora and can perform two types of tasks:\n",
      "\n",
      "1. **Self-supervised representation learning**: This involves using various methods to generate representations of input data, such as token-based or document-level encodings, that capture the underlying structure of the data.\n",
      "2. **Training with contrastive objectives**: This involves pre-training LLMs on a large corpus and then using contrastive objectives to learn to encode document-document similarity.\n",
      "\n",
      "Additionally, the text mentions an \"extra insight\" into how these models can generalize from seen instructions to unseen ones, which is referred to as \"zero-shot generalization\". \n",
      "\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten2212 = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten2212.generate.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=3,\n",
    "    single_prompt=\"What does the following text describe: {chunk}\",\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])\n",
    "    print(item.generative.text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\n",
      "standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with Instructions”. They ﬁne-tuned dense encoders that can also encode task-speciﬁc instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above. Zero-Shot Dense Retrieval The tasks of zero- shot (dense) retrieval are arguably empirically de- ﬁned by Thakur et al. (2021) for the neural re- trieval community. Their BEIR benchmark con- sists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and\n",
      "bold . models bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the Cohere model is still experimental without much detail disclosed. We can only tentatively hypothesize that training techniques may have also played some role in the performance difference. 5.2 HyDE with Fine-tuned Encoder To begin with, HyDE with ﬁne-tuned encoder is notthe intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present. Here we are interested to ﬁnd out if and howHyDE embedding can affect ﬁne-tuned en- coders. In Table 4, we see that less powerful instruc- tion LMs can negatively impact the overall perfor- mance of the ﬁne-tuned retriever. (To remind our readers,ContrieverFTis in-domain supervisedly ﬁne-tuned\n",
      "There is no information provided about unsupervised learning in the text you've shared. However, I can summarize the content for you:\n",
      "\n",
      "Unsupervised learning refers to a type of machine learning where there are no labeled data points. In contrast, supervised learning involves training models on labeled data, where the model learns to predict or classify labels based on examples.\n",
      "\n",
      "The provided text does not mention unsupervised learning at all. It appears to be discussing three related topics:\n",
      "\n",
      "1. Supervised sequence-to-sequence learning (e.g., BERT)\n",
      "2. Task-aware retrieval with instructions\n",
      "3. Zero-shot dense retrieval\n",
      "\n",
      "These topics are mentioned in the context of other papers and research, but there is no mention of unsupervised learning.\n"
     ]
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "\n",
    "ten2212 = papers.with_tenant(\"2212-10496\")\n",
    "\n",
    "response = ten2212.generate.near_text(\n",
    "    query=\"Unsupervised learning\",\n",
    "    limit=3,\n",
    "    grouped_task=\"Explain how unsupervised learning works. Use only the provided content.\",\n",
    "    grouped_properties=[\"chunk\"]\n",
    ")\n",
    "\n",
    "for item in response.objects:\n",
    "    print(item.properties[\"chunk\"])\n",
    "\n",
    "print(response.generative.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_rag(paper_id, query, prompt):\n",
    "    papers = client.collections.get(\"Papers\")\n",
    "    ten = papers.with_tenant(paper_id)\n",
    "\n",
    "    response = ten.generate.near_text(\n",
    "        query=query,\n",
    "        limit=3,\n",
    "        grouped_task=prompt + \" Use only the provided content.\",\n",
    "        grouped_properties=[\"chunk\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"title\": response.objects[0].properties[\"title\"],\n",
    "        \"source\": [p.properties[\"chunk\"] for p in response.objects],\n",
    "        \"generated\": response.generative.text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Precise Zero-Shot Dense Retrieval without Relevance Labels',\n",
       " 'source': ['2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8440– 8451, Online. Association for Computational Lin- guistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020a. Overview of the trec 2019 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. V oorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)',\n",
       "  'standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with Instructions”. They ﬁne-tuned dense encoders that can also encode task-speciﬁc instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above. Zero-Shot Dense Retrieval The tasks of zero- shot (dense) retrieval are arguably empirically de- ﬁned by Thakur et al. (2021) for the neural re- trieval community. Their BEIR benchmark con- sists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and',\n",
       "  'bold . models bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the Cohere model is still experimental without much detail disclosed. We can only tentatively hypothesize that training techniques may have also played some role in the performance difference. 5.2 HyDE with Fine-tuned Encoder To begin with, HyDE with ﬁne-tuned encoder is notthe intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present. Here we are interested to ﬁnd out if and howHyDE embedding can affect ﬁne-tuned en- coders. In Table 4, we see that less powerful instruc- tion LMs can negatively impact the overall perfor- mance of the ﬁne-tuned retriever. (To remind our readers,ContrieverFTis in-domain supervisedly ﬁne-tuned'],\n",
       " 'generated': \"There is no information provided about unsupervised learning in the text you've shared. The content appears to be related to supervised sequence-to-sequence learning and reinforcement learning, specifically discussing tasks such as zero-shot dense retrieval, task-aware retrieval with instructions, and the use of instruction following generative LLMs.\\n\\nHowever, I can provide a general overview of unsupervised learning:\\n\\nUnsupervised learning is a type of machine learning where the algorithm is not given any prior information about the data it's being trained on. In contrast to supervised learning, which requires labeled data to train the model, unsupervised learning relies solely on the patterns and relationships within the data itself.\\n\\nIn unsupervised learning, algorithms are designed to discover hidden structures or relationships in the data that may not be immediately apparent. This can involve clustering similar data points together, identifying clusters of outliers, or even discovering new patterns or features in the data.\\n\\nUnsupervised learning is often used for tasks such as:\\n\\n* Clustering: grouping similar data points into clusters\\n* Dimensionality reduction: reducing the number of features in a dataset while preserving the most important information\\n* Feature extraction: identifying unique features or patterns in the data that can be useful for classification, regression, or other machine learning tasks\\n\\nIn the context of the provided text, the authors are discussing unsupervised sequence-to-sequence learning and reinforcement learning, specifically exploring how these approaches can be adapted to handle task-aware retrieval with instructions and zero-shot dense retrieval.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_rag(\n",
    "    \"2212-10496\",\n",
    "    \"Unsupervised learning\",\n",
    "    \"Explain how unsupervised learning works\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2401-00107': TenantOutput(name='2401-00107', activityStatusInternal=<TenantActivityStatus.ACTIVE: 'ACTIVE'>, activityStatus=<_TenantActivistatusServerValues.HOT: 'HOT'>),\n",
       " '2212-10496': TenantOutput(name='2212-10496', activityStatusInternal=<TenantActivityStatus.ACTIVE: 'ACTIVE'>, activityStatus=<_TenantActivistatusServerValues.HOT: 'HOT'>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = client.collections.get(\"Papers\")\n",
    "papers.tenants.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
